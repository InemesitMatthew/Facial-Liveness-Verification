more info on how to detect faces



FROM WHAT IM SEEING THROUGH TESTING I THINK WE SHOULD MAKE IT LIVE DETECTION NOT WAITING FOR THE IMAGE TO BE TAKEN
like when the face is detected we can draw a rectangle around it showing the face is withing the bounding box

packages we can look into: image,path_provider

import 'dart:math';

import 'package:flutter/material.dart';
import 'package:flutter/services.dart' as services;
import 'package:google_mlkit_commons/google_mlkit_commons.dart';

/// A face detector that detects faces in a given [InputImage].
class FaceDetector {
  static const services.MethodChannel _channel =
      services.MethodChannel('google_mlkit_face_detector');

  /// The options for the face detector.
  final FaceDetectorOptions options;

  /// Instance id.
  final id = DateTime.now().microsecondsSinceEpoch.toString();

  /// Constructor to create an instance of [FaceDetector].
  FaceDetector({required this.options});

  /// Processes the given image for face detection.
  Future<List<Face>> processImage(InputImage inputImage) async {
    final result = await _channel.invokeListMethod<dynamic>(
        'vision#startFaceDetector', <String, dynamic>{
      'options': options.toJson(),
      'id': id,
      'imageData': inputImage.toJson(),
    });

    final List<Face> faces = <Face>[];
    for (final dynamic json in result!) {
      faces.add(Face.fromJson(json));
    }

    return faces;
  }

  /// Closes the detector and releases its resources.
  Future<void> close() =>
      _channel.invokeMethod<void>('vision#closeFaceDetector', {'id': id});
}

/// Immutable options for configuring features of [FaceDetector].
///
/// Used to configure features such as classification, face tracking, speed,
/// etc.
class FaceDetectorOptions {
  /// Constructor for [FaceDetectorOptions].
  ///
  /// The parameter [minFaceSize] must be between 0.0 and 1.0, inclusive.
  FaceDetectorOptions({
    this.enableClassification = false,
    this.enableLandmarks = false,
    this.enableContours = false,
    this.enableTracking = false,
    this.minFaceSize = 0.1,
    this.performanceMode = FaceDetectorMode.fast,
  })  : assert(minFaceSize >= 0.0),
        assert(minFaceSize <= 1.0);

  /// Whether to run additional classifiers for characterizing attributes.
  ///
  /// E.g. "smiling" and "eyes open".
  final bool enableClassification;

  /// Whether to detect [FaceLandmark]s.
  final bool enableLandmarks;

  /// Whether to detect [FaceContour]s.
  final bool enableContours;

  /// Whether to enable face tracking.
  ///
  /// If enabled, the detector will maintain a consistent ID for each face when
  /// processing consecutive frames.
  final bool enableTracking;

  /// The smallest desired face size.
  ///
  /// Expressed as a proportion of the width of the head to the image width.
  ///
  /// Must be a value between 0.0 and 1.0.
  final double minFaceSize;

  /// Option for controlling additional accuracy / speed trade-offs.
  final FaceDetectorMode performanceMode;

  /// Returns a json representation of an instance of [FaceDetectorOptions].
  Map<String, dynamic> toJson() => {
        'enableClassification': enableClassification,
        'enableLandmarks': enableLandmarks,
        'enableContours': enableContours,
        'enableTracking': enableTracking,
        'minFaceSize': minFaceSize,
        'mode': performanceMode.name,
      };
}

/// A human face detected in an image.
class Face {
  /// The axis-aligned bounding rectangle of the detected face.
  ///
  /// The point (0, 0) is defined as the upper-left corner of the image.
  final Rect boundingBox;

  /// The rotation of the face about the horizontal axis of the image.
  ///
  /// Represented in degrees.
  ///
  /// A face with a positive Euler X angle is turned to the camera's up and down.
  ///
  final double? headEulerAngleX;

  /// The rotation of the face about the vertical axis of the image.
  ///
  /// Represented in degrees.
  ///
  /// A face with a positive Euler Y angle is turned to the camera's right and
  /// to its left.
  ///
  /// The Euler Y angle is guaranteed only when using the "accurate" mode
  /// setting of the face detector (as opposed to the "fast" mode setting, which
  /// takes some shortcuts to make detection faster).
  final double? headEulerAngleY;

  /// The rotation of the face about the axis pointing out of the image.
  ///
  /// Represented in degrees.
  ///
  /// A face with a positive Euler Z angle is rotated counter-clockwise relative
  /// to the camera.
  ///
  /// ML Kit always reports the Euler Z angle of a detected face.
  final double? headEulerAngleZ;

  /// Probability that the face's left eye is open.
  ///
  /// A value between 0.0 and 1.0 inclusive, or null if probability was not
  /// computed.
  final double? leftEyeOpenProbability;

  /// Probability that the face's right eye is open.
  ///
  /// A value between 0.0 and 1.0 inclusive, or null if probability was not
  /// computed.
  final double? rightEyeOpenProbability;

  /// Probability that the face is smiling.
  ///
  /// A value between 0.0 and 1.0 inclusive, or null if probability was not
  /// computed.
  final double? smilingProbability;

  /// The tracking ID if the tracking is enabled.
  ///
  /// Null if tracking was not enabled.
  final int? trackingId;

  /// Gets the landmark based on the provided [FaceLandmarkType].
  ///
  /// Null if landmark was not detected.
  final Map<FaceLandmarkType, FaceLandmark?> landmarks;

  /// Gets the contour based on the provided [FaceContourType].
  ///
  /// Null if contour was not detected.
  final Map<FaceContourType, FaceContour?> contours;

  Face({
    required this.boundingBox,
    required this.landmarks,
    required this.contours,
    this.headEulerAngleX,
    this.headEulerAngleY,
    this.headEulerAngleZ,
    this.leftEyeOpenProbability,
    this.rightEyeOpenProbability,
    this.smilingProbability,
    this.trackingId,
  });

  /// Returns an instance of [Face] from a given [json].
  factory Face.fromJson(Map<dynamic, dynamic> json) => Face(
        boundingBox: RectJson.fromJson(json['rect']),
        headEulerAngleX: json['headEulerAngleX'],
        headEulerAngleY: json['headEulerAngleY'],
        headEulerAngleZ: json['headEulerAngleZ'],
        leftEyeOpenProbability: json['leftEyeOpenProbability'],
        rightEyeOpenProbability: json['rightEyeOpenProbability'],
        smilingProbability: json['smilingProbability'],
        trackingId: json['trackingId'],
        landmarks: Map<FaceLandmarkType, FaceLandmark?>.fromIterables(
            FaceLandmarkType.values,
            FaceLandmarkType.values.map((FaceLandmarkType type) {
          final List<dynamic>? pos = json['landmarks'][type.name];
          return (pos == null)
              ? null
              : FaceLandmark(
                  type: type,
                  position: Point<int>(pos[0].toInt(), pos[1].toInt()),
                );
        })),
        contours: Map<FaceContourType, FaceContour?>.fromIterables(
            FaceContourType.values,
            FaceContourType.values.map((FaceContourType type) {
          /// added empty map to pass the tests
          final List<dynamic>? arr =
              (json['contours'] ?? <String, dynamic>{})[type.name];
          return (arr == null)
              ? null
              : FaceContour(
                  type: type,
                  points: arr
                      .map<Point<int>>((dynamic pos) =>
                          Point<int>(pos[0].toInt(), pos[1].toInt()))
                      .toList(),
                );
        })),
      );
}

/// A landmark on a human face detected in an image.
///
/// A landmark is a point on a detected face, such as an eye, nose, or mouth.
class FaceLandmark {
  /// The [FaceLandmarkType] of this landmark.
  final FaceLandmarkType type;

  /// Gets a 2D point for landmark position.
  ///
  /// The point (0, 0) is defined as the upper-left corner of the image.
  final Point<int> position;

  FaceLandmark({required this.type, required this.position});
}

/// A contour on a human face detected in an image.
///
/// Contours of facial features.
class FaceContour {
  /// The [FaceContourType] of this contour.
  final FaceContourType type;

  /// Gets a 2D point [List] for contour positions.
  ///
  /// The point (0, 0) is defined as the upper-left corner of the image.
  final List<Point<int>> points;

  FaceContour({required this.type, required this.points});
}

/// Option for controlling additional trade-offs in performing face detection.
///
/// Accurate tends to detect more faces and may be more precise in determining
/// values such as position, at the cost of speed.
enum FaceDetectorMode {
  accurate,
  fast,
}

/// Available face landmarks detected by [FaceDetector].
enum FaceLandmarkType {
  bottomMouth,
  rightMouth,
  leftMouth,
  rightEye,
  leftEye,
  rightEar,
  leftEar,
  rightCheek,
  leftCheek,
  noseBase,
}

/// Available face contour types detected by [FaceDetector].
enum FaceContourType {
  face,
  leftEyebrowTop,
  leftEyebrowBottom,
  rightEyebrowTop,
  rightEyebrowBottom,
  leftEye,
  rightEye,
  upperLipTop,
  upperLipBottom,
  lowerLipTop,
  lowerLipBottom,
  noseBridge,
  noseBottom,
  leftCheek,
  rightCheek
}

Face detection

bookmark_border



With ML Kit's face detection API, you can detect faces in an image, identify key facial features, and get the contours of detected faces. Note that the API detects faces, it does not recognize people .

With face detection, you can get the information you need to perform tasks like embellishing selfies and portraits, or generating avatars from a user's photo. Because ML Kit can perform face detection in real time, you can use it in applications like video chat or games that respond to the player's expressions.

iOS Android

Key capabilities
Recognize and locate facial features Get the coordinates of the eyes, ears, cheeks, nose, and mouth of every face detected.
Get the contours of facial features Get the contours of detected faces and their eyes, eyebrows, lips, and nose.
Recognize facial expressions Determine whether a person is smiling or has their eyes closed.
Track faces across video frames Get an identifier for each unique detected face. The identifier is consistent across invocations, so you can perform image manipulation on a particular person in a video stream.
Process video frames in real time Face detection is performed on the device, and is fast enough to be used in real-time applications, such as video manipulation.
Example results
Example 1
Physicist Stephen Hawking in Zero Gravity from NASA

For each face detected:

Face 1 of 3
Bounding polygon	(884.880004882812, 149.546676635742), (1030.77197265625, 149.546676635742), (1030.77197265625, 329.660278320312), (884.880004882812, 329.660278320312)
Angles of rotation	Y: -14.054030418395996, Z: -55.007488250732422
Tracking ID	2
Facial landmarks	
Left eye	(945.869323730469, 211.867126464844)
Right eye	(971.579467773438, 247.257247924805)
Bottom of mouth	(907.756591796875, 259.714477539062)
... etc.

Feature probabilities	
Smiling	0.88979166746139526
Left eye open	0.98635888937860727
Right eye open	0.99258323386311531
Example 2 (face contour detection)
When you have face contour detection enabled, you also get a list of points for each facial feature that was detected. These points represent the shape of the feature. The following image illustrates how these points map to a face. Click the image to enlarge it:



Facial feature contours
Nose bridge	(505.149811, 221.201797), (506.987122, 313.285919)
Left eye	(404.642029, 232.854431), (408.527283, 231.366623), (413.565796, 229.427856), (421.378296, 226.967682), (432.598755, 225.434143), (442.953064, 226.089508), (453.899811, 228.594818), (461.516418, 232.650467), (465.069580, 235.600845), (462.170410, 236.316147), (456.233643, 236.891602), (446.363922, 237.966888), (435.698914, 238.149323), (424.320740, 237.235168), (416.037720, 236.012115), (409.983459, 234.870300)
Top of upper lip	(421.662048, 354.520813), (428.103882, 349.694061), (440.847595, 348.048737), (456.549988, 346.295532), (480.526489, 346.089294), (503.375702, 349.470459), (525.624634, 347.352783), (547.371155, 349.091980), (560.082031, 351.693268), (570.226685, 354.210175), (575.305420, 359.257751)
(etc.)	

Face detection concepts

bookmark_border

Face detection locates human faces in visual media such as digital images or video. When a face is detected it has an associated position, size, and orientation; and it can be searched for landmarks such as the eyes and nose.

Here are some of the terms that we use regarding the face detection feature of ML Kit:

Face tracking extends face detection to video sequences. Any face that appears in a video for any length of time can be tracked from frame to frame. This means a face detected in consecutive video frames can be identified as being the same person. Note that this isn't a form of face recognition; face tracking only makes inferences based on the position and motion of the faces in a video sequence.

A landmark is a point of interest within a face. The left eye, right eye, and base of the nose are all examples of landmarks. ML Kit provides the ability to find landmarks on a detected face.

A contour is a set of points that follow the shape of a facial feature. ML Kit provides the ability to find the contours of a face.

Classification determines whether a certain facial characteristic is present. For example, a face can be classified by whether its eyes are open or closed, or if the face is smiling or not.

Face orientation
The following terms describe the angle a face is oriented with respect to the camera:

Euler X: A face with a positive Euler X angle is facing upward.
Euler Y: A face with a positive Euler Y angle is looking to the right of the camera, or looking to the left if negative.
Euler Z: A face with a positive Euler Z angle is rotated counter-clockwise relative to the camera.
ML Kit doesn't report the Euler X, Euler Y or Euler Z angle of a detected face when LANDMARK_MODE_NONE, CONTOUR_MODE_ALL, CLASSIFICATION_MODE_NONE and PERFORMANCE_MODE_FASTare set together.

Landmarks
A landmark is a point of interest within a face. The left eye, right eye, and nose base are all examples of landmarks.

ML Kit detects faces without looking for landmarks. Landmark detection is an optional step that is disabled by default.

The following table summarizes all of the landmarks that can be detected given the Euler Y angle of an associated face:

Euler Y angle	Detectable landmarks
< -36 degrees	left eye, left mouth, left ear, nose base, left cheek
-36 degrees to -12 degrees	left mouth, nose base, bottom mouth, right eye, left eye, left cheek, left ear tip
-12 degrees to 12 degrees	right eye, left eye, nose base, left cheek, right cheek, left mouth, right mouth, bottom mouth
12 degrees to 36 degrees	right mouth, nose base, bottom mouth, left eye, right eye, right cheek, right ear tip
> 36 degrees	right eye, right mouth, right ear, nose base, right cheek
Each detected landmark includes its associated position in the image.

Contours
A contour is a set of points that represent the shape of a facial feature. The following image illustrates how these points map to a face. Click the image to enlarge it:



Each feature contour that ML Kit detects is represented by a fixed number of points:

Face oval	36 points	Upper lip (top)	11 points
Left eyebrow (top)	5 points	Upper lip (bottom)	9 points
Left eyebrow (bottom)	5 points	Lower lip (top)	9 points
Right eyebrow (top)	5 points	Lower lip (bottom)	9 points
Right eyebrow (bottom)	5 points	Nose bridge	2 points
Left eye	16 points	Nose bottom	3 points
Right eye	16 points
Left cheek (center)	1 point
Right cheek (center)	1 points
When you get all of a face's contours at once, you get an array of 133 points, which map to feature contours as shown below:

Indexes of feature contours
0-35	Face oval
36-40	Left eyebrow (top)
41-45	Left eyebrow (bottom)
46-50	Right eyebrow (top)
51-55	Right eyebrow (bottom)
56-71	Left eye
72-87	Right eye
88-96	Upper lip (bottom)
97-105	Lower lip (top)
106-116	Upper lip (top)
117-125	Lower lip (bottom)
126, 127	Nose bridge
128-130	Nose bottom (note that the center point is at index 128)
131	Left cheek (center)
132	Right cheek (center)
Classification
Classification determines whether a certain facial characteristic is present. ML Kit currently supports two classifications: eyes open and smiling.

Classification is a certainty value. It indicates the confidence that a facial characteristic is present. For example, a value of 0.7 or more for the smiling classification indicates that it's likely that a person is smiling.

Both of these classifications rely upon landmark detection.

Also note that the classifications "eyes open" and "smiling" only work for frontal faces, i.e., faces with a small Euler Y angle (between -18 and 18 degrees).

Minimum Face Size
The minimum face size is the desired face size, expressed as the ratio of the width of the head to the width of the image. For example, the value of 0.1 means that the smallest face to search for is roughly 10% of the width of the image being searched.

The minimum face size is a performance vs. accuracy trade-off: setting the minimum size smaller lets the detector find smaller faces but detection will take longer; setting it larger might exclude smaller faces but will run faster.

The minimum face size is not a hard limit; the detector may find faces slightly smaller than specified.

Detect faces with ML Kit on Android

bookmark_border

You can use ML Kit to detect faces in images and video.

This API is available using either an unbundled library that must be downloaded before use or a bundled library that increases your app size. See this guide for more information on the differences between the two installation options.
Feature	Unbundled	Bundled
Implementation	Model is dynamically downloaded via Google Play Services.	Model is statically linked to your app at build time.
App size	About 800 KB size increase.	About 6.9 MB size increase.
Initialization time	Might have to wait for model to download before first use.	Model is available immediately
Try it out
Play around with the sample app to see an example usage of this API.
Try the code yourself with the codelab.
Before you begin
This API requires Android API level 21 or above. Make sure that your app's build file uses a minSdkVersion value of 21 or higher.
In your project-level build.gradle file, make sure to include Google's Maven repository in both your buildscript and allprojects sections.

Add the dependencies for the ML Kit Android libraries to your module's app-level gradle file, which is usually app/build.gradle. Choose one of the following dependencies based on your needs:

For bundling the model with your app:


dependencies {
  // ...
  // Use this dependency to bundle the model with your app
  implementation 'com.google.mlkit:face-detection:16.1.7'
}
For using the model in Google Play Services:


dependencies {
  // ...
  // Use this dependency to use the dynamically downloaded model in Google Play Services
  implementation 'com.google.android.gms:play-services-mlkit-face-detection:17.1.0'
}
If you choose to use the model in Google Play Services, you can configure your app to automatically download the model to the device after your app is installed from the Play Store. To do so, add the following declaration to your app's AndroidManifest.xml file:


<application ...>
      ...
      <meta-data
          android:name="com.google.mlkit.vision.DEPENDENCIES"
          android:value="face" >
      <!-- To use multiple models: android:value="face,model2,model3" -->
</application>
You can also explicitly check the model availability and request download through Google Play services ModuleInstallClient API.

If you don't enable install-time model downloads or request explicit download, the model is downloaded the first time you run the detector. Requests you make before the download has completed produce no results.

Input image guidelines
For face recognition, you should use an image with dimensions of at least 480x360 pixels. For ML Kit to accurately detect faces, input images must contain faces that are represented by sufficient pixel data. In general, each face you want to detect in an image should be at least 100x100 pixels. If you want to detect the contours of faces, ML Kit requires higher resolution input: each face should be at least 200x200 pixels.

If you detect faces in a real-time application, you might also want to consider the overall dimensions of the input images. Smaller images can be processed faster, so to reduce latency, capture images at lower resolutions, but keep in mind the above accuracy requirements and ensure that the subject's face occupies as much of the image as possible. Also see tips to improve real-time performance.

Poor image focus can also impact accuracy. If you don't get acceptable results, ask the user to recapture the image.

The orientation of a face relative to the camera can also affect what facial features ML Kit detects. See Face Detection Concepts.

1. Configure the face detector
Before you apply face detection to an image, if you want to change any of the face detector's default settings, specify those settings with a FaceDetectorOptions object. You can change the following settings:
Settings
setPerformanceMode	PERFORMANCE_MODE_FAST (default) | PERFORMANCE_MODE_ACCURATE
Favor speed or accuracy when detecting faces.

setLandmarkMode	LANDMARK_MODE_NONE (default) | LANDMARK_MODE_ALL
Whether to attempt to identify facial "landmarks": eyes, ears, nose, cheeks, mouth, and so on.

setContourMode	CONTOUR_MODE_NONE (default) | CONTOUR_MODE_ALL
Whether to detect the contours of facial features. Contours are detected for only the most prominent face in an image.

setClassificationMode	CLASSIFICATION_MODE_NONE (default) | CLASSIFICATION_MODE_ALL
Whether or not to classify faces into categories such as "smiling", and "eyes open".

setMinFaceSize	float (default: 0.1f)
Sets the smallest desired face size, expressed as the ratio of the width of the head to width of the image.

enableTracking	false (default) | true
Whether or not to assign faces an ID, which can be used to track faces across images.

Note that when contour detection is enabled, only one face is detected, so face tracking doesn't produce useful results. For this reason, and to improve detection speed, don't enable both contour detection and face tracking.

For example:

Kotlin
Java

// High-accuracy landmark detection and face classification
val highAccuracyOpts = FaceDetectorOptions.Builder()
        .setPerformanceMode(FaceDetectorOptions.PERFORMANCE_MODE_ACCURATE)
        .setLandmarkMode(FaceDetectorOptions.LANDMARK_MODE_ALL)
        .setClassificationMode(FaceDetectorOptions.CLASSIFICATION_MODE_ALL)
        .build()

// Real-time contour detection
val realTimeOpts = FaceDetectorOptions.Builder()
        .setContourMode(FaceDetectorOptions.CONTOUR_MODE_ALL)
        .build()
2. Prepare the input image
To detect faces in an image, create an InputImage object from either a Bitmap, media.Image, ByteBuffer, byte array, or a file on the device. Then, pass the InputImage object to the FaceDetector's process method.
For face detection, you should use an image with dimensions of at least 480x360 pixels. If you are detecting faces in real time, capturing frames at this minimum resolution can help reduce latency.

You can create an InputImage object from different sources, each is explained below.

Using a media.Image
To create an InputImage object from a media.Image object, such as when you capture an image from a device's camera, pass the media.Image object and the image's rotation to InputImage.fromMediaImage().

If you use the CameraX library, the OnImageCapturedListener and ImageAnalysis.Analyzer classes calculate the rotation value for you.

Kotlin
Java

private class YourImageAnalyzer : ImageAnalysis.Analyzer {

    override fun analyze(imageProxy: ImageProxy) {
        val mediaImage = imageProxy.image
        if (mediaImage != null) {
            val image = InputImage.fromMediaImage(mediaImage, imageProxy.imageInfo.rotationDegrees)
            // Pass image to an ML Kit Vision API
            // ...
        }
    }
}
If you don't use a camera library that gives you the image's rotation degree, you can calculate it from the device's rotation degree and the orientation of camera sensor in the device:

Kotlin
Java

private val ORIENTATIONS = SparseIntArray()

init {
    ORIENTATIONS.append(Surface.ROTATION_0, 0)
    ORIENTATIONS.append(Surface.ROTATION_90, 90)
    ORIENTATIONS.append(Surface.ROTATION_180, 180)
    ORIENTATIONS.append(Surface.ROTATION_270, 270)
}

/**
 * Get the angle by which an image must be rotated given the device's current
 * orientation.
 */
@RequiresApi(api = Build.VERSION_CODES.LOLLIPOP)
@Throws(CameraAccessException::class)
private fun getRotationCompensation(cameraId: String, activity: Activity, isFrontFacing: Boolean): Int {
    // Get the device's current rotation relative to its "native" orientation.
    // Then, from the ORIENTATIONS table, look up the angle the image must be
    // rotated to compensate for the device's rotation.
    val deviceRotation = activity.windowManager.defaultDisplay.rotation
    var rotationCompensation = ORIENTATIONS.get(deviceRotation)

    // Get the device's sensor orientation.
    val cameraManager = activity.getSystemService(CAMERA_SERVICE) as CameraManager
    val sensorOrientation = cameraManager
            .getCameraCharacteristics(cameraId)
            .get(CameraCharacteristics.SENSOR_ORIENTATION)!!

    if (isFrontFacing) {
        rotationCompensation = (sensorOrientation + rotationCompensation) % 360
    } else { // back-facing
        rotationCompensation = (sensorOrientation - rotationCompensation + 360) % 360
    }
    return rotationCompensation
}
Then, pass the media.Image object and the rotation degree value to InputImage.fromMediaImage():

Kotlin
Java

val image = InputImage.fromMediaImage(mediaImage, rotation)

Using a file URI
To create an InputImage object from a file URI, pass the app context and file URI to InputImage.fromFilePath(). This is useful when you use an ACTION_GET_CONTENT intent to prompt the user to select an image from their gallery app.

Kotlin
Java

val image: InputImage
try {
    image = InputImage.fromFilePath(context, uri)
} catch (e: IOException) {
    e.printStackTrace()
}

Using a ByteBuffer or ByteArray
To create an InputImage object from a ByteBuffer or a ByteArray, first calculate the image rotation degree as previously described for media.Image input. Then, create the InputImage object with the buffer or array, together with image's height, width, color encoding format, and rotation degree:

Kotlin
Java

val image = InputImage.fromByteBuffer(
        byteBuffer,
        /* image width */ 480,
        /* image height */ 360,
        rotationDegrees,
        InputImage.IMAGE_FORMAT_NV21 // or IMAGE_FORMAT_YV12
)

// Or:
val image = InputImage.fromByteArray(
        byteArray,
        /* image width */ 480,
        /* image height */ 360,
        rotationDegrees,
        InputImage.IMAGE_FORMAT_NV21 // or IMAGE_FORMAT_YV12
)
Using a Bitmap
To create an InputImage object from a Bitmap object, make the following declaration:

Kotlin
Java

val image = InputImage.fromBitmap(bitmap, 0)

The image is represented by a Bitmap object together with rotation degrees.

3. Get an instance of FaceDetector
Kotlin
Java

val detector = FaceDetection.getClient(options)
// Or, to use the default option:
// val detector = FaceDetection.getClient();
4. Process the image
Pass the image to the process method:
Kotlin
Java

val result = detector.process(image)
        .addOnSuccessListener { faces ->
            // Task completed successfully
            // ...
        }
        .addOnFailureListener { e ->
            // Task failed with an exception
            // ...
        }
Note: If you are using the CameraX API, make sure to close the ImageProxy when finish using it, e.g., by adding an OnCompleteListener to the Task returned from the process method. See the VisionProcessorBase class in the quickstart sample app for an example.
5. Get information about detected faces
If the face detection operation succeeds, a list of Face objects are passed to the success listener. Each Face object represents a face that was detected in the image. For each face, you can get its bounding coordinates in the input image, as well as any other information you configured the face detector to find. For example:
Kotlin
Java

for (face in faces) {
    val bounds = face.boundingBox
    val rotY = face.headEulerAngleY // Head is rotated to the right rotY degrees
    val rotZ = face.headEulerAngleZ // Head is tilted sideways rotZ degrees

    // If landmark detection was enabled (mouth, ears, eyes, cheeks, and
    // nose available):
    val leftEar = face.getLandmark(FaceLandmark.LEFT_EAR)
    leftEar?.let {
        val leftEarPos = leftEar.position
    }

    // If contour detection was enabled:
    val leftEyeContour = face.getContour(FaceContour.LEFT_EYE)?.points
    val upperLipBottomContour = face.getContour(FaceContour.UPPER_LIP_BOTTOM)?.points

    // If classification was enabled:
    if (face.smilingProbability != null) {
        val smileProb = face.smilingProbability
    }
    if (face.rightEyeOpenProbability != null) {
        val rightEyeOpenProb = face.rightEyeOpenProbability
    }

    // If face tracking was enabled:
    if (face.trackingId != null) {
        val id = face.trackingId
    }
}
Example of face contours
When you have face contour detection enabled, you get a list of points for each facial feature that was detected. These points represent the shape of the feature. See Face Detection Concepts for details about how contours are represented.

The following image illustrates how these points map to a face, click the image to enlarge it:

example detected face contour mesh
Real-time face detection
If you want to use face detection in a real-time application, follow these guidelines to achieve the best framerates:

Configure the face detector to use either face contour detection or classification and landmark detection, but not both:

 Contour detection
 Landmark detection
 Classification
 Landmark detection and classification
 Contour detection and landmark detection
 Contour detection and classification
 Contour detection, landmark detection, and classification

Enable FAST mode (enabled by default).

Consider capturing images at a lower resolution. However, also keep in mind this API's image dimension requirements.

If you use the Camera or camera2 API, throttle calls to the detector. If a new video frame becomes available while the detector is running, drop the frame. See the VisionProcessorBase class in the quickstart sample app for an example.
If you use the CameraX API, be sure that backpressure strategy is set to its default value ImageAnalysis.STRATEGY_KEEP_ONLY_LATEST. This guarantees only one image will be delivered for analysis at a time. If more images are produced when the analyzer is busy, they will be dropped automatically and not queued for delivery. Once the image being analyzed is closed by calling ImageProxy.close(), the next latest image will be delivered.
If you use the output of the detector to overlay graphics on the input image, first get the result from ML Kit, then render the image and overlay in a single step. This renders to the display surface only once for each input frame. See the CameraSourcePreview and GraphicOverlay classes in the quickstart sample app for an example.
If you use the Camera2 API, capture images in ImageFormat.YUV_420_888 format. If you use the older Camera API, capture images in ImageFormat.NV21 format.

google_mlkit_face_detection 0.13.1 copy "google_mlkit_face_detection: ^0.13.1" to clipboard
Published 5 months ago • verified publisherflutter-ml.devDart 3 compatible
SDKFlutterPlatformAndroidiOS
283
Readme
Changelog
Example
Installing
Versions
Scores
Google's ML Kit Face Detection for Flutter 
Pub Version analysis Star on Github License: MIT

A Flutter plugin to use Google's ML Kit Face Detection to detect faces in an image, identify key facial features, and get the contours of detected faces.

PLEASE READ THIS before continuing or posting a new issue:

Google's ML Kit was build only for mobile platforms: iOS and Android apps. Web or any other platform is not supported, you can request support for those platform to Google in their repo.

This plugin is not sponsored or maintained by Google. The authors are developers excited about Machine Learning that wanted to expose Google's native APIs to Flutter.

Google's ML Kit APIs are only developed natively for iOS and Android. This plugin uses Flutter Platform Channels as explained here.

Messages are passed between the client (the app/plugin) and host (platform) using platform channels as illustrated in this diagram:



Messages and responses are passed asynchronously, to ensure the user interface remains responsive. To read more about platform channels go here.

Because this plugin uses platform channels, no Machine Learning processing is done in Flutter/Dart, all the calls are passed to the native platform using MethodChannel in Android and FlutterMethodChannel in iOS, and executed using Google's native APIs. Think of this plugin as a bridge between your app and Google's native ML Kit APIs. This plugin only passes the call to the native API and the processing is done by Google's API. It is important that you understand this concept when it comes to debugging errors for your ML model and/or app.

Since the plugin uses platform channels, you may encounter issues with the native API. Before submitting a new issue, identify the source of the issue. You can run both iOS and/or Android native example apps by Google and make sure that the issue is not reproducible with their native examples. If you can reproduce the issue in their apps then report the issue to Google. The authors do not have access to the source code of their native APIs, so you need to report the issue to them. If you find that their example apps are okay and still you have an issue using this plugin, then look at our closed and open issues. If you cannot find anything that can help you then report the issue and provide enough details. Be patient, someone from the community will eventually help you.

Requirements 
iOS 
Minimum iOS Deployment Target: 15.5
Xcode 15.3.0 or newer
Swift 5
ML Kit does not support 32-bit architectures (i386 and armv7). ML Kit does support 64-bit architectures (x86_64 and arm64). Check this list to see if your device has the required device capabilities. More info here.
Since ML Kit does not support 32-bit architectures (i386 and armv7), you need to exclude armv7 architectures in Xcode in order to run flutter build ios or flutter build ipa. More info here.

Go to Project > Runner > Building Settings > Excluded Architectures > Any SDK > armv7



Your Podfile should look like this:

platform :ios, '15.5'  # or newer version

...

# add this line:
$iOSVersion = '15.5'  # or newer version

post_install do |installer|
  # add these lines:
  installer.pods_project.build_configurations.each do |config|
    config.build_settings["EXCLUDED_ARCHS[sdk=*]"] = "armv7"
    config.build_settings['IPHONEOS_DEPLOYMENT_TARGET'] = $iOSVersion
  end

  installer.pods_project.targets.each do |target|
    flutter_additional_ios_build_settings(target)

    # add these lines:
    target.build_configurations.each do |config|
      if Gem::Version.new($iOSVersion) > Gem::Version.new(config.build_settings['IPHONEOS_DEPLOYMENT_TARGET'])
        config.build_settings['IPHONEOS_DEPLOYMENT_TARGET'] = $iOSVersion
      end
    end

  end
end
Notice that the minimum IPHONEOS_DEPLOYMENT_TARGET is 15.5, you can set it to something newer but not older.

Android 
minSdkVersion: 21
targetSdkVersion: 35
compileSdkVersion: 35
Usage 
Face Detection 
Create an instance of InputImage
Create an instance of InputImage as explained here.

final InputImage inputImage;
Create an instance of FaceDetector
final options = FaceDetectorOptions();
final faceDetector = FaceDetector(options: options);
Process image
final List<Face> faces = await faceDetector.processImage(inputImage);

for (Face face in faces) {
  final Rect boundingBox = face.boundingBox;

  final double? rotX = face.headEulerAngleX; // Head is tilted up and down rotX degrees
  final double? rotY = face.headEulerAngleY; // Head is rotated to the right rotY degrees
  final double? rotZ = face.headEulerAngleZ; // Head is tilted sideways rotZ degrees

  // If landmark detection was enabled with FaceDetectorOptions (mouth, ears,
  // eyes, cheeks, and nose available):
  final FaceLandmark? leftEar = face.landmarks[FaceLandmarkType.leftEar];
  if (leftEar != null) {
    final Point<int> leftEarPos = leftEar.position;
  }

  // If classification was enabled with FaceDetectorOptions:
  if (face.smilingProbability != null) {
    final double? smileProb = face.smilingProbability;
  }

  // If face tracking was enabled with FaceDetectorOptions:
  if (face.trackingId != null) {
    final int? id = face.trackingId;
  }
}
Release resources with close()
faceDetector.close();

Flutter Liveness Detection with ML Kit: Simplified and Cool
Muhammad Ridwan
Muhammad Ridwan

Follow
6 min read
·
Jan 4, 2025
6






Press enter or click to view image in full size

Liveness detection is a must-have for secure apps these days, especially for extra identity verification. In this guide, we’ll dive into how to build a Flutter Liveness Detection app using Google’s ML Kit for face detection. As Alan Kay once said,

“The best way to predict the future is to invent it.” — Alan Kay

So, let’s keep it simple, fun, and to the point. Ready? Let’s roll!

Setting Up Permissions
First things first — let’s get your app to ask for camera permissions. In your AndroidManifest.xml, add this line:

<uses-permission android:name="android.permission.CAMERA" />
Also, if you’re targeting Android 12 or above, include this in the <application> tag:

<queries>
    <intent>
        <action android:name="android.media.action.IMAGE_CAPTURE" />
    </intent>
</queries>
Gradle Tweaks
Next, update your build.gradle files to include the necessary dependencies. Add ML Kit’s face detection:

In android/app/build.gradle, make sure these dependencies are included:

dependencies {
    implementation 'com.google.mlkit:face-detection:16.1.7'
}
Let’s Code It!
Now for the fun part — writing some code! We’ll create a simple app where users perform actions like smiling or blinking to verify their identity.

Main File (main.dart)

import 'package:flutter/material.dart';
import 'package:flutter/services.dart';
import 'package:permission_handler/permission_handler.dart';
import 'face_detection_page.dart';
import 'package:camera/camera.dart';

void main() async {
  WidgetsFlutterBinding.ensureInitialized();
  await requestCameraPermission();

  runApp(const MyApp());
}

/// Requests camera permission from the user.
Future<void> requestCameraPermission() async {
  final status = await Permission.camera.request();
  if (!status.isGranted) {
    // Handle permission denial
    runApp(const PermissionDeniedApp());
  }
}

class PermissionDeniedApp extends StatelessWidget {
  const PermissionDeniedApp({super.key});

  @override
  Widget build(BuildContext context) {
    return MaterialApp(
      home: Scaffold(
        appBar: AppBar(
          title: const Text("Permission Denied"),
        ),
        body: Center(
          child: AlertDialog(
            title: const Text("Permission Denied"),
            content: const Text("Camera access is required for verification."),
            actions: [
              TextButton(
                onPressed: () => SystemNavigator.pop(),
                child: const Text("OK"),
              ),
            ],
          ),
        ),
      ),
    );
  }
}

class MyApp extends StatelessWidget {
  const MyApp({super.key});

  @override
  Widget build(BuildContext context) {
    return const MaterialApp(
      title: 'Material App',
      home: HomePage(),
    );
  }
}

class HomePage extends StatelessWidget {
  const HomePage({super.key});

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      backgroundColor: Colors.white,
      appBar: AppBar(
        backgroundColor: Colors.amberAccent,
        toolbarHeight: 70,
        centerTitle: true,
        title: const Text('Verify Your Identity'),
      ),
      body: Center(
        child: Column(
          mainAxisAlignment: MainAxisAlignment.center,
          children: [
            const Text('Please click the button below to start verification',
                textAlign: TextAlign.center, style: TextStyle(fontSize: 20)),
            const SizedBox(height: 30),
            ElevatedButton(
              style: ElevatedButton.styleFrom(
                padding: const EdgeInsets.all(20),
                shape: RoundedRectangleBorder(
                  borderRadius: BorderRadius.circular(15),
                ),
                foregroundColor: Colors.black,
                backgroundColor: Colors.amberAccent,
              ),
              onPressed: () async {
                final cameras = await availableCameras();
                if (cameras.isNotEmpty) {
                  final result = await Navigator.push(
                    context,
                    MaterialPageRoute(
                      builder: (context) => const FaceDetectionPage(),
                    ),
                  );
                  if (result == true) {
                    ScaffoldMessenger.of(context).showSnackBar(
                      const SnackBar(content: Text('Verification Successful!')),
                    );
                  }
                } else {
                  ScaffoldMessenger.of(context).showSnackBar(
                    const SnackBar(content: Text('Camera not active!')),
                  );
                }
              },
              child: const Text(
                'Verify Now',
                style: TextStyle(fontWeight: FontWeight.bold, fontSize: 20),
              ),
            ),
          ],
        ),
      ),
    );
  }
}
Face Detection Page (face_detection_page.dart)

import 'package:flutter/material.dart';
import 'package:flutter/services.dart';
import 'package:google_mlkit_face_detection/google_mlkit_face_detection.dart';
import 'package:camera/camera.dart';

class FaceDetectionPage extends StatefulWidget {
  const FaceDetectionPage({super.key});

  @override
  _FaceDetectionPageState createState() => _FaceDetectionPageState();
}

class _FaceDetectionPageState extends State<FaceDetectionPage> {
  final FaceDetector faceDetector = FaceDetector(
    options: FaceDetectorOptions(
        enableContours: true,
        enableClassification: true,
        minFaceSize: 0.3,
        performanceMode: FaceDetectorMode.fast),
  );

  late CameraController cameraController;
  bool isCameraInitialized = false;
  bool isDetecting = false;
  bool isFrontCamera = true;
  List<String> challengeActions = ['smile', 'blink', 'lookRight', 'lookLeft'];
  int currentActionIndex = 0;
  bool waitingForNeutral = false;

  double? smilingProbability;
  double? leftEyeOpenProbability;
  double? rightEyeOpenProbability;
  double? headEulerAngleY;

  @override
  void initState() {
    super.initState();
    initializeCamera();
    challengeActions.shuffle();
  }

  // Initialize the camera controller
  Future<void> initializeCamera() async {
    final cameras = await availableCameras();
    final frontCamera = cameras.firstWhere(
        (camera) => camera.lensDirection == CameraLensDirection.front);
    cameraController = CameraController(frontCamera, ResolutionPreset.high,
        enableAudio: false);
    await cameraController.initialize();
    if (mounted) {
      setState(() {
        isCameraInitialized = true;
      });
      startFaceDetection();
    }
  }

  // Start face detection on the camera image stream
  void startFaceDetection() {
    if (isCameraInitialized) {
      cameraController.startImageStream((CameraImage image) {
        if (!isDetecting) {
          isDetecting = true;
          detectFaces(image).then((_) {
            isDetecting = false;
          });
        }
      });
    }
  }

  // Detect faces in the camera image
  Future<void> detectFaces(CameraImage image) async {
    try {
      final WriteBuffer allBytes = WriteBuffer();
      for (Plane plane in image.planes) {
        allBytes.putUint8List(plane.bytes);
      }
      final bytes = allBytes.done().buffer.asUint8List();

      final inputImage = InputImage.fromBytes(
        bytes: bytes,
        metadata: InputImageMetadata(
          size: Size(image.width.toDouble(), image.height.toDouble()),
          rotation: InputImageRotation.rotation270deg,
          format: InputImageFormat.nv21,
          bytesPerRow: image.planes[0].bytesPerRow,
        ),
      );

      final faces = await faceDetector.processImage(inputImage);

      if (!mounted) return;

      if (faces.isNotEmpty) {
        final face = faces.first;
        setState(() {
          smilingProbability = face.smilingProbability;
          leftEyeOpenProbability = face.leftEyeOpenProbability;
          rightEyeOpenProbability = face.rightEyeOpenProbability;
          headEulerAngleY = face.headEulerAngleY;
        });
        checkChallenge(face);
      }
    } catch (e) {
      debugPrint('Error in face detection: $e');
    }
  }

  // Check if the face is performing the current challenge action
  void checkChallenge(Face face) async {
    if (waitingForNeutral) {
      if (isNeutralPosition(face)) {
        waitingForNeutral = false;
      } else {
        return;
      }
    }

    String currentAction = challengeActions[currentActionIndex];
    bool actionCompleted = false;

    switch (currentAction) {
      case 'smile':
        actionCompleted =
            face.smilingProbability != null && face.smilingProbability! > 0.5;
        break;
      case 'blink':
        actionCompleted = (face.leftEyeOpenProbability != null &&
                face.leftEyeOpenProbability! < 0.3) ||
            (face.rightEyeOpenProbability != null &&
                face.rightEyeOpenProbability! < 0.3);
        break;
      case 'lookRight':
        actionCompleted =
            face.headEulerAngleY != null && face.headEulerAngleY! < -10;
        break;
      case 'lookLeft':
        actionCompleted =
            face.headEulerAngleY != null && face.headEulerAngleY! > 10;
        break;
    }

    if (actionCompleted) {
      currentActionIndex++;
      if (currentActionIndex >= challengeActions.length) {
        currentActionIndex = 0;
        if (mounted) {
          Navigator.pop(context, true);
        }
      } else {
        waitingForNeutral = true;
      }
    }
  }

  // Check if the face is in a neutral position
  bool isNeutralPosition(Face face) {
    return (face.smilingProbability == null ||
            face.smilingProbability! < 0.1) &&
        (face.leftEyeOpenProbability == null ||
            face.leftEyeOpenProbability! > 0.7) &&
        (face.rightEyeOpenProbability == null ||
            face.rightEyeOpenProbability! > 0.7) &&
        (face.headEulerAngleY == null ||
            (face.headEulerAngleY! > -10 && face.headEulerAngleY! < 10));
  }

  @override
  void dispose() {
    cameraController.stopImageStream();
    faceDetector.close();
    cameraController.dispose();
    super.dispose();
  }

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      appBar: AppBar(
        backgroundColor: Colors.amberAccent,
        toolbarHeight: 70,
        centerTitle: true,
        title: const Text("Verify Your Identity"),
      ),
      body: isCameraInitialized
          ? Stack(
              children: [
                Positioned.fill(
                  child: CameraPreview(cameraController),
                ),
                CustomPaint(
                  painter: HeadMaskPainter(),
                  child: Container(),
                ),
                Positioned(
                  top: 16,
                  left: 16,
                  right: 16,
                  child: Container(
                    padding: const EdgeInsets.all(8),
                    color: Colors.black54,
                    child: Column(
                      children: [
                        Text(
                          'Please ${getActionDescription(challengeActions[currentActionIndex])}',
                          style: const TextStyle(
                              color: Colors.white, fontSize: 18),
                          textAlign: TextAlign.center,
                        ),
                        const SizedBox(height: 8),
                        Text(
                          'Step ${currentActionIndex + 1} of ${challengeActions.length}',
                          style: const TextStyle(
                              color: Colors.white, fontSize: 16),
                          textAlign: TextAlign.center,
                        ),
                      ],
                    ),
                  ),
                ),
                Positioned(
                  bottom: 16,
                  left: 16,
                  child: Container(
                    padding: const EdgeInsets.all(8),
                    color: Colors.black54,
                    child: Column(
                      crossAxisAlignment: CrossAxisAlignment.start,
                      children: [
                        Text(
                          'Smile: ${smilingProbability != null ? (smilingProbability! * 100).toStringAsFixed(2) : 'N/A'}%',
                          style: const TextStyle(color: Colors.white),
                        ),
                        Text(
                          'Blink: ${leftEyeOpenProbability != null && rightEyeOpenProbability != null ? (((leftEyeOpenProbability! + rightEyeOpenProbability!) / 2) * 100).toStringAsFixed(2) : 'N/A'}%',
                          style: const TextStyle(color: Colors.white),
                        ),
                        Text(
                          'Look: ${headEulerAngleY != null ? headEulerAngleY!.toStringAsFixed(2) : 'N/A'}°',
                          style: const TextStyle(color: Colors.white),
                        ),
                      ],
                    ),
                  ),
                ),
              ],
            )
          : const Center(child: CircularProgressIndicator()),
    );
  }

  // Get the description of the current challenge action
  String getActionDescription(String action) {
    switch (action) {
      case 'smile':
        return 'smile';
      case 'blink':
        return 'blink';
      case 'lookRight':
        return 'look right';
      case 'lookLeft':
        return 'look left';
      default:
        return '';
    }
  }
}

// Custom painter for head mask
class HeadMaskPainter extends CustomPainter {
  @override
  void paint(Canvas canvas, Size size) {
    final paint = Paint()
      ..color = Colors.black.withOpacity(0.5)
      ..style = PaintingStyle.fill;

    final center = Offset(size.width / 2, size.height / 2);
    final radius = size.width * 0.4;

    final path = Path()
      ..addRect(Rect.fromLTWH(0, 0, size.width, size.height))
      ..addOval(Rect.fromCircle(center: center, radius: radius))
      ..fillType = PathFillType.evenOdd;

    canvas.drawPath(path, paint);
  }

  @override
  bool shouldRepaint(covariant CustomPainter oldDelegate) {
    return false;
  }
}
The Concept: ML Kit Face Detection
ML Kit’s face detection API uses machine learning to identify faces, their contours, and key actions like smiles and blinks. We leverage this to create simple challenges for verifying liveness. This way, it’s tough to fake using just a static image — adding an extra layer of security!

Press enter or click to view image in full size

And that’s it! Now you have a working Flutter app with liveness detection. Try it out and secure your apps with confidence. 🚀


CONTEXT FROM DEEPSEEK: 
# Comprehensive Guide: Implementing Live Facial Liveness Check in Flutter

This guide will walk you through implementing a live facial liveness detection system in Flutter using Google's ML Kit for face detection. The solution will perform real-time analysis to verify that a real person is present (not a photo or video).

## Overview

Liveness detection helps prevent spoofing attacks by verifying that the face being detected is from a live person. We'll implement challenges like smiling, blinking, and head movements to confirm liveness.

## Prerequisites

1. Flutter SDK (v3.0.0 or newer)
2. Android Studio/Xcode for platform-specific setup
3. Physical device with camera (emulators won't work for camera functionality)

## Step 1: Add Dependencies

Add the following dependencies to your `pubspec.yaml`:

```yaml
dependencies:
  flutter:
    sdk: flutter
  camera: ^0.10.0
  google_mlkit_face_detection: ^1.1.0
  permission_handler: ^11.0.0
  vector_math: ^2.1.0
```

Run `flutter pub get` to install the packages.

## Step 2: Configure Platform-Specific Settings

### Android Configuration

1. Update `android/app/build.gradle`:
   ```gradle
   android {
      defaultConfig {
          minSdkVersion 21
          targetSdkVersion 33
          // ...
      }
      // ...
   }
   ```

2. Add permissions to `android/app/src/main/AndroidManifest.xml`:
   ```xml
   <uses-permission android:name="android.permission.CAMERA" />
   <uses-feature android:name="android.hardware.camera" />
   ```

### iOS Configuration

1. Update `ios/Podfile`:
   ```ruby
   platform :ios, '15.5'
   post_install do |installer|
     installer.pods_project.build_configurations.each do |config|
       config.build_settings["EXCLUDED_ARCHS[sdk=*]"] = "armv7"
       config.build_settings['IPHONEOS_DEPLOYMENT_TARGET'] = '15.5'
     end
   end
   ```

2. Add permissions to `ios/Runner/Info.plist`:
   ```xml
   <key>NSCameraUsageDescription</key>
   <string>This app needs camera access for face verification</string>
   ```

## Step 3: Implement Camera Service

Create a `camera_service.dart` file:

```dart
import 'dart:async';
import 'dart:ui' as ui;

import 'package:camera/camera.dart';
import 'package:flutter/foundation.dart';
import 'package:flutter/services.dart';

class CameraService {
  CameraController? _controller;
  List<CameraDescription>? _cameras;
  Completer<void> _completer = Completer<void>();

  Future<void> initialize() async {
    _cameras = await availableCameras();
    _controller = CameraController(
      _cameras!.firstWhere(
        (camera) => camera.lensDirection == CameraLensDirection.front,
      ),
      ResolutionPreset.high,
      enableAudio: false,
    );

    await _controller!.initialize();
    _completer.complete();
  }

  Future<CameraController> getController() async {
    await _completer.future;
    return _controller!;
  }

  Future<void> startStream(Function(CameraImage) onImageAvailable) async {
    await _completer.future;
    return _controller!.startImageStream(onImageAvailable);
  }

  Future<void> dispose() async {
    await _controller?.dispose();
    _controller = null;
  }

  Future<ui.Image> convertCameraImageToUiImage(CameraImage cameraImage) async {
    final WriteBuffer allBytes = WriteBuffer();
    for (Plane plane in cameraImage.planes) {
      allBytes.putUint8List(plane.bytes);
    }
    final bytes = allBytes.done().buffer.asUint8List();

    final Completer<ui.Image> completer = Completer();
    ui.decodeImageFromList(bytes, (ui.Image img) {
      completer.complete(img);
    });

    return completer.future;
  }
}
```

## Step 4: Implement Face Detection Service

Create a `face_detector_service.dart` file:

```dart
import 'dart:async';
import 'dart:ui' as ui;

import 'package:camera/camera.dart';
import 'package:google_mlkit_face_detection/google_mlkit_face_detection.dart';

class FaceDetectorService {
  final FaceDetector _faceDetector;
  bool _isProcessing = false;

  FaceDetectorService()
      : _faceDetector = FaceDetector(
          options: FaceDetectorOptions(
            enableContours: true,
            enableClassification: true,
            enableLandmarks: true,
            enableTracking: true,
            minFaceSize: 0.3,
            performanceMode: FaceDetectorMode.fast,
          ),
        );

  Future<List<Face>> detectFacesFromCameraImage(CameraImage image) async {
    if (_isProcessing) return [];
    _isProcessing = true;

    try {
      final WriteBuffer allBytes = WriteBuffer();
      for (final Plane plane in image.planes) {
        allBytes.putUint8List(plane.bytes);
      }
      final bytes = allBytes.done().buffer.asUint8List();

      final inputImage = InputImage.fromBytes(
        bytes: bytes,
        inputImageData: InputImageData(
          size: Size(image.width.toDouble(), image.height.toDouble()),
          imageRotation: InputImageRotation.rotation270deg,
          inputImageFormat: InputImageFormat.nv21,
          planeData: image.planes.map(
            (Plane plane) {
              return InputImagePlaneMetadata(
                bytesPerRow: plane.bytesPerRow,
                height: plane.height,
                width: plane.width,
              );
            },
          ).toList(),
        ),
      );

      final faces = await _faceDetector.processImage(inputImage);
      _isProcessing = false;
      return faces;
    } catch (e) {
      _isProcessing = false;
      return [];
    }
  }

  Future<void> dispose() async {
    await _faceDetector.close();
  }
}
```

## Step 5: Implement Liveness Detection Logic

Create a `liveness_detector.dart` file:

```dart
import 'package:google_mlkit_face_detection/google_mlkit_face_detection.dart';

class LivenessDetector {
  final List<LivenessChallenge> _challenges;
  int _currentChallengeIndex = 0;
  DateTime? _lastActionTime;
  bool _waitingForNeutral = false;

  LivenessDetector()
      : _challenges = [
          LivenessChallenge('smile', 'Please smile'),
          LivenessChallenge('blink', 'Please blink'),
          LivenessChallenge('turnRight', 'Please turn your head to the right'),
          LivenessChallenge('turnLeft', 'Please turn your head to the left'),
        ]..shuffle();

  LivenessResult checkLiveness(Face face) {
    if (_currentChallengeIndex >= _challenges.length) {
      return LivenessResult(true, 'All challenges completed');
    }

    final currentChallenge = _challenges[_currentChallengeIndex];

    // Check if we're waiting for the user to return to neutral
    if (_waitingForNeutral) {
      if (_isNeutralPosition(face)) {
        _waitingForNeutral = false;
        _currentChallengeIndex++;
        if (_currentChallengeIndex < _challenges.length) {
          return LivenessResult(
            false,
            _challenges[_currentChallengeIndex].instruction,
          );
        } else {
          return LivenessResult(true, 'Verification complete');
        }
      }
      return LivenessResult(false, 'Return to neutral position');
    }

    // Check the current challenge
    bool challengeCompleted = false;
    switch (currentChallenge.type) {
      case 'smile':
        challengeCompleted = face.smilingProbability != null &&
            face.smilingProbability! > 0.7;
        break;
      case 'blink':
        challengeCompleted = (face.leftEyeOpenProbability != null &&
                face.leftEyeOpenProbability! < 0.3) ||
            (face.rightEyeOpenProbability != null &&
                face.rightEyeOpenProbability! < 0.3);
        break;
      case 'turnRight':
        challengeCompleted = face.headEulerAngleY != null &&
            face.headEulerAngleY! < -15;
        break;
      case 'turnLeft':
        challengeCompleted =
            face.headEulerAngleY != null && face.headEulerAngleY! > 15;
        break;
    }

    if (challengeCompleted) {
      _waitingForNeutral = true;
      _lastActionTime = DateTime.now();
      return LivenessResult(false, 'Return to neutral position');
    }

    return LivenessResult(false, currentChallenge.instruction);
  }

  bool _isNeutralPosition(Face face) {
    return (face.smilingProbability == null ||
            face.smilingProbability! < 0.3) &&
        (face.leftEyeOpenProbability == null ||
            face.leftEyeOpenProbability! > 0.7) &&
        (face.rightEyeOpenProbability == null ||
            face.rightEyeOpenProbability! > 0.7) &&
        (face.headEulerAngleY == null ||
            (face.headEulerAngleY! > -10 && face.headEulerAngleY! < 10));
  }

  void reset() {
    _currentChallengeIndex = 0;
    _lastActionTime = null;
    _waitingForNeutral = false;
    _challenges.shuffle();
  }

  int get remainingChallenges => _challenges.length - _currentChallengeIndex;
}

class LivenessChallenge {
  final String type;
  final String instruction;

  LivenessChallenge(this.type, this.instruction);
}

class LivenessResult {
  final bool isComplete;
  final String message;

  LivenessResult(this.isComplete, this.message);
}
```

## Step 6: Create the Main Liveness Detection Widget

Create a `liveness_detection_widget.dart` file:

```dart
import 'dart:async';
import 'dart:ui' as ui;

import 'package:camera/camera.dart';
import 'package:flutter/material.dart';
import 'package:google_mlkit_face_detection/google_mlkit_face_detection.dart';

import 'camera_service.dart';
import 'face_detector_service.dart';
import 'liveness_detector.dart';

class LivenessDetectionWidget extends StatefulWidget {
  final Function(bool) onVerificationComplete;

  const LivenessDetectionWidget({Key? key, required this.onVerificationComplete})
      : super(key: key);

  @override
  _LivenessDetectionWidgetState createState() =>
      _LivenessDetectionWidgetState();
}

class _LivenessDetectionWidgetState extends State<LivenessDetectionWidget> {
  final CameraService _cameraService = CameraService();
  final FaceDetectorService _faceDetectorService = FaceDetectorService();
  final LivenessDetector _livenessDetector = LivenessDetector();

  List<Face> _faces = [];
  String _instruction = 'Please position your face in the frame';
  bool _isProcessing = false;
  bool _isVerificationComplete = false;
  StreamSubscription<CameraImage>? _cameraSubscription;

  @override
  void initState() {
    super.initState();
    _initializeCamera();
  }

  Future<void> _initializeCamera() async {
    try {
      await _cameraService.initialize();
      _cameraSubscription = _cameraService.startStream(_processCameraImage);
    } catch (e) {
      ScaffoldMessenger.of(context).showSnackBar(
        SnackBar(content: Text('Failed to initialize camera: $e')),
      );
    }
  }

  Future<void> _processCameraImage(CameraImage image) async {
    if (_isProcessing || _isVerificationComplete) return;
    _isProcessing = true;

    try {
      final faces = await _faceDetectorService.detectFacesFromCameraImage(image);
      
      if (mounted) {
        setState(() {
          _faces = faces;
          
          if (faces.isNotEmpty) {
            final result = _livenessDetector.checkLiveness(faces.first);
            _instruction = result.message;
            
            if (result.isComplete && !_isVerificationComplete) {
              _isVerificationComplete = true;
              widget.onVerificationComplete(true);
            }
          }
        });
      }
    } catch (e) {
      print('Error processing image: $e');
    } finally {
      _isProcessing = false;
    }
  }

  @override
  void dispose() {
    _cameraSubscription?.cancel();
    _cameraService.dispose();
    _faceDetectorService.dispose();
    super.dispose();
  }

  @override
  Widget build(BuildContext context) {
    return Stack(
      children: [
        FutureBuilder<CameraController>(
          future: _cameraService.getController(),
          builder: (context, snapshot) {
            if (snapshot.hasData) {
              return CameraPreview(snapshot.data!);
            } else {
              return const Center(child: CircularProgressIndicator());
            }
          },
        ),
        _buildFaceOverlay(),
        _buildInstructionOverlay(),
      ],
    );
  }

  Widget _buildFaceOverlay() {
    return CustomPaint(
      painter: FaceOutlinePainter(faces: _faces),
      child: Container(),
    );
  }

  Widget _buildInstructionOverlay() {
    return Positioned(
      top: 50,
      left: 0,
      right: 0,
      child: Container(
        padding: const EdgeInsets.all(16),
        color: Colors.black54,
        child: Text(
          _instruction,
          style: const TextStyle(color: Colors.white, fontSize: 18),
          textAlign: TextAlign.center,
        ),
      ),
    );
  }
}

class FaceOutlinePainter extends CustomPainter {
  final List<Face> faces;

  FaceOutlinePainter({required this.faces});

  @override
  void paint(Canvas canvas, Size size) {
    final paint = Paint()
      ..color = Colors.green
      ..style = PaintingStyle.stroke
      ..strokeWidth = 2.0;

    for (final face in faces) {
      final boundingBox = face.boundingBox;
      
      // Convert the bounding box to screen coordinates
      final left = boundingBox.left;
      final top = boundingBox.top;
      final right = boundingBox.right;
      final bottom = boundingBox.bottom;
      
      canvas.drawRect(
        Rect.fromLTRB(left, top, right, bottom),
        paint,
      );
    }
  }

  @override
  bool shouldRepaint(covariant CustomPainter oldDelegate) {
    return true;
  }
}
```

## Step 7: Implement the Main Application

Update your `main.dart` file:

```dart
import 'package:flutter/material.dart';
import 'package:permission_handler/permission_handler.dart';

import 'liveness_detection_widget.dart';

void main() {
  runApp(const MyApp());
}

class MyApp extends StatelessWidget {
  const MyApp({Key? key}) : super(key: key);

  @override
  Widget build(BuildContext context) {
    return MaterialApp(
      title: 'Face Liveness Detection',
      theme: ThemeData(
        primarySwatch: Colors.blue,
      ),
      home: const HomePage(),
    );
  }
}

class HomePage extends StatefulWidget {
  const HomePage({Key? key}) : super(key: key);

  @override
  _HomePageState createState() => _HomePageState();
}

class _HomePageState extends State<HomePage> {
  bool _isVerified = false;
  bool _isChecking = false;

  Future<void> _checkPermissions() async {
    final status = await Permission.camera.request();
    if (!status.isGranted) {
      ScaffoldMessenger.of(context).showSnackBar(
        const SnackBar(content: Text('Camera permission is required')),
      );
    }
  }

  @override
  void initState() {
    super.initState();
    _checkPermissions();
  }

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      appBar: AppBar(
        title: const Text('Face Liveness Detection'),
      ),
      body: Center(
        child: _isChecking
            ? LivenessDetectionWidget(
                onVerificationComplete: (isVerified) {
                  setState(() {
                    _isChecking = false;
                    _isVerified = isVerified;
                  });
                },
              )
            : Column(
                mainAxisAlignment: MainAxisAlignment.center,
                children: [
                  Text(
                    _isVerified
                        ? 'Verification Successful!'
                        : 'Verify your identity',
                    style: const TextStyle(fontSize: 24),
                  ),
                  const SizedBox(height: 20),
                  ElevatedButton(
                    onPressed: () {
                      setState(() {
                        _isChecking = true;
                        _isVerified = false;
                      });
                    },
                    child: const Text('Start Verification'),
                  ),
                ],
              ),
      ),
    );
  }
}
```

## Step 8: Testing and Optimization

1. **Test on real devices**: Camera functionality requires physical devices
2. **Adjust thresholds**: Fine-tune the probability thresholds based on testing
3. **Performance optimization**: 
   - Reduce image resolution if processing is too slow
   - Implement frame skipping to reduce processing load
   - Use isolate for face detection to avoid UI jank

## Advanced Enhancements

1. **Add visual feedback**: Implement a face position guide to help users position themselves correctly
2. **Add timeout handling**: Implement a timeout for each challenge
3. **Add security features**: 
   - Check for multiple faces
   - Implement anti-spoofing techniques like texture analysis
4. **Add backend integration**: Send verification results to your server for additional processing

## Troubleshooting Common Issues

1. **Camera not initializing**: Check permissions and ensure you're using a physical device
2. **Slow performance**: Reduce the camera resolution or implement frame skipping
3. **Face detection accuracy**: Adjust the `minFaceSize` parameter or lighting conditions

## Conclusion

This implementation provides a solid foundation for live facial liveness detection in Flutter. The solution uses Google's ML Kit for accurate face detection and implements challenge-response verification to ensure liveness.

Remember to test thoroughly in various lighting conditions and with different devices to ensure reliability. You can extend this implementation with additional security measures based on your specific requirements.

CONTEXT FROM CLaude
Based on your documentation and current best practices, I've created a comprehensive guide for implementing live facial liveness detection in Flutter using ML Kit. The guide covers everything from basic setup to advanced security measures.

Key highlights of the implementation:

**Security Features:**
- Multi-challenge verification with randomized sequences
- Anti-spoofing measures including face tracking validation
- Temporal analysis to detect unnatural movements
- Quality checks for face size and lighting conditions

**Performance Optimizations:**
- Frame skipping to reduce processing load
- Memory management with periodic cleanup
- Optimal camera settings for face detection
- Efficient image processing pipeline

**User Experience:**
- Clear visual guidance with animated overlays
- Progress indicators and real-time feedback
- Timeout mechanisms with countdown timers
- Proper error handling and recovery

The implementation includes several improvements over the basic example in your documentation:

1. **Enhanced Challenge System**: More sophisticated validation requiring consecutive frames and smooth transitions
2. **Better Error Handling**: Comprehensive error management for camera, permissions, and ML Kit failures
3. **Anti-Spoofing**: Advanced validation to prevent photo/video attacks
4. **Production Ready**: Includes proper resource cleanup, memory management, and performance optimizations

The code is structured for maintainability with separate services for liveness detection logic and anti-spoofing validation. This makes it easier to customize challenge types, adjust thresholds, and add new security measures.

Would you like me to elaborate on any specific aspect of the implementation, such as adding more challenge types, implementing server-side verification, or optimizing for specific use cases?

# Complete Guide: Flutter Live Facial Liveness Detection with ML Kit

## Table of Contents
1. [Overview & Architecture](#overview--architecture)
2. [Prerequisites & Setup](#prerequisites--setup)
3. [Project Configuration](#project-configuration)
4. [Implementation](#implementation)
5. [Security Considerations](#security-considerations)
6. [Performance Optimization](#performance-optimization)
7. [Troubleshooting](#troubleshooting)
8. [Best Practices](#best-practices)

## Overview & Architecture

### What is Liveness Detection?
Liveness detection is a security technique that verifies whether a face being presented to a camera belongs to a living person rather than a photograph, video, or 3D model. ML Kit's face detection API uses machine learning to identify faces, their contours, and key actions like smiles and blinks to create challenges for verifying liveness.

### How It Works
1. **Real-time Face Detection**: Uses ML Kit to continuously detect faces in camera stream
2. **Challenge-Response**: Prompts user to perform specific actions (smile, blink, turn head)
3. **Feature Analysis**: Analyzes facial features and movements to verify authenticity
4. **Sequential Validation**: Requires completion of multiple randomized actions

### Architecture Components
```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Camera Feed   │───▶│   ML Kit Face   │───▶│   Challenge     │
│                 │    │   Detection     │    │   Validation    │
└─────────────────┘    └─────────────────┘    └─────────────────┘
        │                        │                        │
        ▼                        ▼                        ▼
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   UI Overlay    │    │   Face Analysis │    │   Success/Fail  │
│                 │    │   (Probability) │    │   Callback      │
└─────────────────┘    └─────────────────┘    └─────────────────┘
```

## Prerequisites & Setup

### Required Dependencies
Add these to your `pubspec.yaml`:

```yaml
dependencies:
  flutter:
    sdk: flutter
  camera: ^0.11.0+2
  google_mlkit_face_detection: ^0.13.1
  permission_handler: ^11.3.1
  path_provider: ^2.1.4
```

### Platform Requirements

#### Android
- **minSdkVersion**: 21
- **targetSdkVersion**: 35
- **compileSdkVersion**: 35

#### iOS
- **Minimum iOS Deployment Target**: 15.5
- **Xcode**: 15.3.0 or newer
- **Swift**: 5
- **Architecture**: 64-bit only (excludes armv7)

## Project Configuration

### Android Setup

#### 1. Permissions (`android/app/src/main/AndroidManifest.xml`)
```xml
<manifest xmlns:android="http://schemas.android.com/apk/res/android">
    <!-- Camera Permission -->
    <uses-permission android:name="android.permission.CAMERA" />
    
    <!-- Android 12+ Camera Intent Support -->
    <queries>
        <intent>
            <action android:name="android.media.action.IMAGE_CAPTURE" />
        </intent>
    </queries>
    
    <application
        android:label="liveness_detection"
        android:name="${applicationName}"
        android:icon="@mipmap/ic_launcher">
        
        <!-- ML Kit Model Download Configuration -->
        <meta-data
            android:name="com.google.mlkit.vision.DEPENDENCIES"
            android:value="face" />
    </application>
</manifest>
```

#### 2. Gradle Configuration (`android/app/build.gradle`)
```gradle
android {
    compileSdkVersion 35
    ndkVersion "25.1.8937393"

    compileOptions {
        sourceCompatibility JavaVersion.VERSION_1_8
        targetCompatibility JavaVersion.VERSION_1_8
    }

    kotlinOptions {
        jvmTarget = '1.8'
    }

    defaultConfig {
        applicationId "com.example.liveness_detection"
        minSdkVersion 21
        targetSdkVersion 35
        versionCode 1
        versionName "1.0"
    }
}

dependencies {
    implementation 'com.google.mlkit:face-detection:16.1.7'
}
```

### iOS Setup

#### 1. Podfile Configuration
```ruby
# Uncomment this line to define a global platform for your project
platform :ios, '15.5'

# CocoaPods analytics sends network stats synchronously affecting flutter build latency.
ENV['COCOAPODS_DISABLE_STATS'] = 'true'

$iOSVersion = '15.5'

project 'Runner', {
  'Debug' => :debug,
  'Profile' => :release,
  'Release' => :release,
}

def flutter_root
  generated_xcode_build_settings_path = File.expand_path(File.join('..', 'Flutter', 'Generated.xcconfig'), __FILE__)
  unless File.exist?(generated_xcode_build_settings_path)
    raise "#{generated_xcode_build_settings_path} must exist. If you're running pod install manually, make sure flutter pub get is executed first"
  end

  File.foreach(generated_xcode_build_settings_path) do |line|
    matches = line.match(/FLUTTER_ROOT\=(.*)/)
    return matches[1].strip if matches
  end
  raise "FLUTTER_ROOT not found in #{generated_xcode_build_settings_path}. Try deleting Generated.xcconfig, then run flutter pub get"
end

require File.expand_path(File.join('packages', 'flutter_tools', 'bin', 'podhelper'), flutter_root)

flutter_ios_podfile_setup

target 'Runner' do
  use_frameworks!
  use_modular_headers!

  flutter_install_all_ios_pods File.dirname(File.realpath(__FILE__))
  target 'RunnerTests' do
    inherit! :search_paths
  end
end

post_install do |installer|
  installer.pods_project.build_configurations.each do |config|
    config.build_settings["EXCLUDED_ARCHS[sdk=*]"] = "armv7"
    config.build_settings['IPHONEOS_DEPLOYMENT_TARGET'] = $iOSVersion
  end

  installer.pods_project.targets.each do |target|
    flutter_additional_ios_build_settings(target)
    
    target.build_configurations.each do |config|
      if Gem::Version.new($iOSVersion) > Gem::Version.new(config.build_settings['IPHONEOS_DEPLOYMENT_TARGET'])
        config.build_settings['IPHONEOS_DEPLOYMENT_TARGET'] = $iOSVersion
      end
    end
  end
end
```

#### 2. Info.plist Configuration (`ios/Runner/Info.plist`)
```xml
<dict>
    <!-- Camera Permission -->
    <key>NSCameraUsageDescription</key>
    <string>This app needs access to camera for face verification</string>
    
    <!-- Exclude armv7 architecture -->
    <key>UIRequiredDeviceCapabilities</key>
    <array>
        <string>arm64</string>
    </array>
</dict>
```

## Implementation

### 1. Main Application Setup

```dart
// main.dart
import 'package:flutter/material.dart';
import 'package:flutter/services.dart';
import 'package:permission_handler/permission_handler.dart';
import 'face_detection_page.dart';
import 'package:camera/camera.dart';

void main() async {
  WidgetsFlutterBinding.ensureInitialized();
  
  // Set preferred orientations
  await SystemChrome.setPreferredOrientations([
    DeviceOrientation.portraitUp,
  ]);
  
  // Request camera permission
  await requestCameraPermission();
  
  runApp(const LivenessDetectionApp());
}

Future<void> requestCameraPermission() async {
  final status = await Permission.camera.request();
  if (!status.isGranted) {
    runApp(const PermissionDeniedApp());
    return;
  }
}

class PermissionDeniedApp extends StatelessWidget {
  const PermissionDeniedApp({super.key});

  @override
  Widget build(BuildContext context) {
    return MaterialApp(
      debugShowCheckedModeBanner: false,
      home: Scaffold(
        appBar: AppBar(
          title: const Text("Permission Required"),
          backgroundColor: Colors.red.shade400,
          foregroundColor: Colors.white,
        ),
        body: Center(
          child: Padding(
            padding: const EdgeInsets.all(24.0),
            child: Column(
              mainAxisAlignment: MainAxisAlignment.center,
              children: [
                Icon(
                  Icons.camera_alt_outlined,
                  size: 80,
                  color: Colors.red.shade400,
                ),
                const SizedBox(height: 24),
                const Text(
                  "Camera Permission Required",
                  style: TextStyle(
                    fontSize: 24,
                    fontWeight: FontWeight.bold,
                  ),
                  textAlign: TextAlign.center,
                ),
                const SizedBox(height: 16),
                const Text(
                  "This app requires camera access to perform face verification. Please grant camera permission in your device settings.",
                  style: TextStyle(fontSize: 16),
                  textAlign: TextAlign.center,
                ),
                const SizedBox(height: 32),
                ElevatedButton.icon(
                  onPressed: () => openAppSettings(),
                  icon: const Icon(Icons.settings),
                  label: const Text("Open Settings"),
                  style: ElevatedButton.styleFrom(
                    backgroundColor: Colors.red.shade400,
                    foregroundColor: Colors.white,
                    padding: const EdgeInsets.symmetric(
                      horizontal: 24,
                      vertical: 12,
                    ),
                  ),
                ),
                TextButton(
                  onPressed: () => SystemNavigator.pop(),
                  child: const Text("Exit App"),
                ),
              ],
            ),
          ),
        ),
      ),
    );
  }
}

class LivenessDetectionApp extends StatelessWidget {
  const LivenessDetectionApp({super.key});

  @override
  Widget build(BuildContext context) {
    return MaterialApp(
      title: 'Liveness Detection',
      debugShowCheckedModeBanner: false,
      theme: ThemeData(
        primarySwatch: Colors.blue,
        useMaterial3: true,
      ),
      home: const HomePage(),
    );
  }
}

class HomePage extends StatelessWidget {
  const HomePage({super.key});

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      backgroundColor: Colors.grey.shade50,
      appBar: AppBar(
        backgroundColor: Colors.blue.shade600,
        foregroundColor: Colors.white,
        elevation: 0,
        centerTitle: true,
        title: const Text(
          'Face Liveness Verification',
          style: TextStyle(fontWeight: FontWeight.bold),
        ),
      ),
      body: Center(
        child: Padding(
          padding: const EdgeInsets.all(24.0),
          child: Column(
            mainAxisAlignment: MainAxisAlignment.center,
            children: [
              Container(
                padding: const EdgeInsets.all(24),
                decoration: BoxDecoration(
                  color: Colors.white,
                  borderRadius: BorderRadius.circular(20),
                  boxShadow: [
                    BoxShadow(
                      color: Colors.grey.shade300,
                      blurRadius: 10,
                      offset: const Offset(0, 5),
                    ),
                  ],
                ),
                child: Column(
                  children: [
                    Icon(
                      Icons.face_retouching_natural,
                      size: 80,
                      color: Colors.blue.shade600,
                    ),
                    const SizedBox(height: 16),
                    const Text(
                      'Verify Your Identity',
                      style: TextStyle(
                        fontSize: 24,
                        fontWeight: FontWeight.bold,
                      ),
                      textAlign: TextAlign.center,
                    ),
                    const SizedBox(height: 12),
                    Text(
                      'We need to verify that you are a real person. This process will take about 30 seconds.',
                      style: TextStyle(
                        fontSize: 16,
                        color: Colors.grey.shade600,
                      ),
                      textAlign: TextAlign.center,
                    ),
                  ],
                ),
              ),
              const SizedBox(height: 32),
              SizedBox(
                width: double.infinity,
                child: ElevatedButton.icon(
                  style: ElevatedButton.styleFrom(
                    backgroundColor: Colors.blue.shade600,
                    foregroundColor: Colors.white,
                    padding: const EdgeInsets.symmetric(vertical: 16),
                    shape: RoundedRectangleBorder(
                      borderRadius: BorderRadius.circular(12),
                    ),
                    elevation: 3,
                  ),
                  onPressed: () async {
                    try {
                      final cameras = await availableCameras();
                      if (cameras.isNotEmpty) {
                        final result = await Navigator.push<bool>(
                          context,
                          MaterialPageRoute(
                            builder: (context) => const FaceDetectionPage(),
                          ),
                        );
                        
                        if (result == true && context.mounted) {
                          _showSuccessDialog(context);
                        } else if (result == false && context.mounted) {
                          _showFailureDialog(context);
                        }
                      } else {
                        if (context.mounted) {
                          _showErrorSnackBar(context, 'No camera available on this device');
                        }
                      }
                    } catch (e) {
                      if (context.mounted) {
                        _showErrorSnackBar(context, 'Failed to initialize camera: $e');
                      }
                    }
                  },
                  icon: const Icon(Icons.camera_alt),
                  label: const Text(
                    'Start Verification',
                    style: TextStyle(
                      fontSize: 18,
                      fontWeight: FontWeight.bold,
                    ),
                  ),
                ),
              ),
            ],
          ),
        ),
      ),
    );
  }

  void _showSuccessDialog(BuildContext context) {
    showDialog(
      context: context,
      barrierDismissible: false,
      builder: (context) => AlertDialog(
        shape: RoundedRectangleBorder(borderRadius: BorderRadius.circular(16)),
        icon: Icon(Icons.check_circle, color: Colors.green.shade600, size: 60),
        title: const Text('Verification Successful!'),
        content: const Text('Your identity has been verified successfully.'),
        actions: [
          TextButton(
            onPressed: () => Navigator.of(context).pop(),
            child: const Text('Continue'),
          ),
        ],
      ),
    );
  }

  void _showFailureDialog(BuildContext context) {
    showDialog(
      context: context,
      barrierDismissible: false,
      builder: (context) => AlertDialog(
        shape: RoundedRectangleBorder(borderRadius: BorderRadius.circular(16)),
        icon: Icon(Icons.error, color: Colors.red.shade600, size: 60),
        title: const Text('Verification Failed'),
        content: const Text('Unable to verify your identity. Please try again.'),
        actions: [
          TextButton(
            onPressed: () => Navigator.of(context).pop(),
            child: const Text('Try Again'),
          ),
        ],
      ),
    );
  }

  void _showErrorSnackBar(BuildContext context, String message) {
    ScaffoldMessenger.of(context).showSnackBar(
      SnackBar(
        content: Text(message),
        backgroundColor: Colors.red.shade600,
        behavior: SnackBarBehavior.floating,
        margin: const EdgeInsets.all(16),
        shape: RoundedRectangleBorder(borderRadius: BorderRadius.circular(8)),
      ),
    );
  }
}
```

### 2. Face Detection Engine

```dart
// face_detection_page.dart
import 'dart:async';
import 'dart:math';
import 'package:flutter/material.dart';
import 'package:flutter/services.dart';
import 'package:google_mlkit_face_detection/google_mlkit_face_detection.dart';
import 'package:camera/camera.dart';

class FaceDetectionPage extends StatefulWidget {
  const FaceDetectionPage({super.key});

  @override
  State<FaceDetectionPage> createState() => _FaceDetectionPageState();
}

class _FaceDetectionPageState extends State<FaceDetectionPage>
    with SingleTickerProviderStateMixin {
  
  // ML Kit Face Detector
  final FaceDetector _faceDetector = FaceDetector(
    options: FaceDetectorOptions(
      enableClassification: true,
      enableLandmarks: true,
      enableContours: true,
      enableTracking: true,
      minFaceSize: 0.3,
      performanceMode: FaceDetectorMode.fast,
    ),
  );

  // Camera Controller
  late CameraController _cameraController;
  bool _isCameraInitialized = false;
  bool _isDetecting = false;
  
  // Challenge System
  final List<String> _challengeActions = [
    'smile',
    'blink', 
    'lookLeft',
    'lookRight',
    'neutral'
  ];
  
  int _currentActionIndex = 0;
  bool _waitingForNeutral = false;
  bool _challengeCompleted = false;
  
  // Face Analysis Data
  double? _smilingProbability;
  double? _leftEyeOpenProbability;
  double? _rightEyeOpenProbability;
  double? _headEulerAngleY;
  double? _headEulerAngleX;
  Rect? _faceBoundingBox;
  
  // UI State
  late AnimationController _animationController;
  Timer? _challengeTimer;
  Timer? _timeoutTimer;
  int _timeRemaining = 30;
  
  // Challenge thresholds
  static const double _smileThreshold = 0.7;
  static const double _eyeClosedThreshold = 0.3;
  static const double _headTurnThreshold = 15.0;
  static const double _neutralSmileThreshold = 0.3;
  static const double _neutralEyeThreshold = 0.7;

  @override
  void initState() {
    super.initState();
    _animationController = AnimationController(
      duration: const Duration(seconds: 1),
      vsync: this,
    )..repeat(reverse: true);
    
    _initializeCamera();
    _shuffleChallenges();
    _startTimeout();
  }

  void _shuffleChallenges() {
    _challengeActions.shuffle(Random());
    // Ensure we start with neutral position
    if (_challengeActions.first != 'neutral') {
      _challengeActions.insert(0, 'neutral');
    }
  }

  void _startTimeout() {
    _timeoutTimer = Timer.periodic(const Duration(seconds: 1), (timer) {
      if (mounted) {
        setState(() {
          _timeRemaining--;
        });
        
        if (_timeRemaining <= 0) {
          _handleVerificationFailure('Verification timeout');
        }
      }
    });
  }

  Future<void> _initializeCamera() async {
    try {
      final cameras = await availableCameras();
      if (cameras.isEmpty) {
        _handleVerificationFailure('No camera available');
        return;
      }

      final frontCamera = cameras.firstWhere(
        (camera) => camera.lensDirection == CameraLensDirection.front,
        orElse: () => cameras.first,
      );

      _cameraController = CameraController(
        frontCamera,
        ResolutionPreset.medium,
        enableAudio: false,
        imageFormatGroup: ImageFormatGroup.nv21,
      );

      await _cameraController.initialize();

      if (mounted) {
        setState(() {
          _isCameraInitialized = true;
        });
        _startFaceDetection();
      }
    } catch (e) {
      _handleVerificationFailure('Failed to initialize camera: $e');
    }
  }

  void _startFaceDetection() {
    if (_isCameraInitialized && !_challengeCompleted) {
      _cameraController.startImageStream(_processImage);
    }
  }

  Future<void> _processImage(CameraImage image) async {
    if (_isDetecting || _challengeCompleted) return;

    _isDetecting = true;
    try {
      final inputImage = _convertCameraImageToInputImage(image);
      if (inputImage != null) {
        final faces = await _faceDetector.processImage(inputImage);
        
        if (mounted) {
          _analyzeFaces(faces);
        }
      }
    } catch (e) {
      debugPrint('Error in face detection: $e');
    } finally {
      _isDetecting = false;
    }
  }

  InputImage? _convertCameraImageToInputImage(CameraImage image) {
    try {
      final WriteBuffer allBytes = WriteBuffer();
      for (final Plane plane in image.planes) {
        allBytes.putUint8List(plane.bytes);
      }
      final bytes = allBytes.done().buffer.asUint8List();

      final imageRotation = _getImageRotation();

      return InputImage.fromBytes(
        bytes: bytes,
        metadata: InputImageMetadata(
          size: Size(image.width.toDouble(), image.height.toDouble()),
          rotation: imageRotation,
          format: InputImageFormat.nv21,
          bytesPerRow: image.planes[0].bytesPerRow,
        ),
      );
    } catch (e) {
      debugPrint('Error converting camera image: $e');
      return null;
    }
  }

  InputImageRotation _getImageRotation() {
    // For front camera, we typically need 270 degrees rotation
    return InputImageRotation.rotation270deg;
  }

  void _analyzeFaces(List<Face> faces) {
    if (faces.isEmpty) {
      setState(() {
        _faceBoundingBox = null;
        _smilingProbability = null;
        _leftEyeOpenProbability = null;
        _rightEyeOpenProbability = null;
        _headEulerAngleY = null;
        _headEulerAngleX = null;
      });
      return;
    }

    final face = faces.first;
    
    setState(() {
      _faceBoundingBox = face.boundingBox;
      _smilingProbability = face.smilingProbability;
      _leftEyeOpenProbability = face.leftEyeOpenProbability;
      _rightEyeOpenProbability = face.rightEyeOpenProbability;
      _headEulerAngleY = face.headEulerAngleY;
      _headEulerAngleX = face.headEulerAngleX;
    });

    _processChallenge(face);
  }

  void _processChallenge(Face face) {
    if (_challengeCompleted) return;

    final currentAction = _challengeActions[_currentActionIndex];
    
    if (_waitingForNeutral) {
      if (_isNeutralPosition(face)) {
        _waitingForNeutral = false;
        _moveToNextChallenge();
      }
      return;
    }

    bool actionCompleted = false;

    switch (currentAction) {
      case 'neutral':
        actionCompleted = _isNeutralPosition(face);
        break;
      case 'smile':
        actionCompleted = _isSmiling(face);
        break;
      case 'blink':
        actionCompleted = _isBlinking(face);
        break;
      case 'lookLeft':
        actionCompleted = _isLookingLeft(face);
        break;
      case 'lookRight':
        actionCompleted = _isLookingRight(face);
        break;
    }

    if (actionCompleted) {
      if (currentAction != 'neutral') {
        _waitingForNeutral = true;
      } else {
        _moveToNextChallenge();
      }
    }
  }

  void _moveToNextChallenge() {
    _currentActionIndex++;
    
    if (_currentActionIndex >= _challengeActions.length) {
      _completeVerification();
    } else {
      // Add small delay between challenges
      Future.delayed(const Duration(milliseconds: 500), () {
        if (mounted) {
          setState(() {
            // Challenge updated
          });
        }
      });
    }
  }

  // Challenge validation methods
  bool _isNeutralPosition(Face face) {
    final smiling = face.smilingProbability ?? 0.0;
    final leftEye = face.leftEyeOpenProbability ?? 1.0;
    final rightEye = face.rightEyeOpenProbability ?? 1.0;
    final headY = face.headEulerAngleY ?? 0.0;

    return smiling < _neutralSmileThreshold &&
           leftEye > _neutralEyeThreshold &&
           rightEye > _neutralEyeThreshold &&
           headY.abs() < 10.0;
  }

  bool _isSmiling(Face face) {
    return (face.smilingProbability ?? 0.0) > _smileThreshold;
  }

  bool _isBlinking(Face face) {
    final leftEye = face.leftEyeOpenProbability ?? 1.0;
    final rightEye = face.rightEyeOpenProbability ?? 1.0;
    
    return leftEye < _eyeClosedThreshold || rightEye < _eyeClosedThreshold;
  }

  bool _isLookingLeft(Face face) {
    return (face.headEulerAngleY ?? 0.0) > _headTurnThreshold;
  }

  bool _isLookingRight(Face face) {
    return (face.headEulerAngleY ?? 0.0) < -_headTurnThreshold;
  }

  void _completeVerification() {
    _challengeCompleted = true;
    _timeoutTimer?.cancel();
    _cameraController.stopImageStream();
    
    // Add success delay for better UX
    Timer(const Duration(milliseconds: 1000), () {
      if (mounted) {
        Navigator.of(context).pop(true);
      }
    });
  }

  void _handleVerificationFailure(String reason) {
    _timeoutTimer?.cancel();
    if (_isCameraInitialized) {
      _cameraController.stopImageStream();
    }
    
    debugPrint('Verification failed: $reason');
    
    if (mounted) {
      Navigator.of(context).pop(false);
    }
  }

  String _getCurrentInstructionText() {
    if (_challengeCompleted) return 'Verification Successful!';
    if (_waitingForNeutral) return 'Return to neutral position';
    
    final action = _challengeActions[_currentActionIndex];
    switch (action) {
      case 'neutral':
        return 'Look straight at the camera';
      case 'smile':
        return 'Please smile';
      case 'blink':
        return 'Please blink';
      case 'lookLeft':
        return 'Look to your left';
      case 'lookRight':
        return 'Look to your right';
      default:
        return 'Follow the instructions';
    }
  }

  @override
  void dispose() {
    _animationController.dispose();
    _timeoutTimer?.cancel();
    _challengeTimer?.cancel();
    
    if (_isCameraInitialized) {
      _cameraController.stopImageStream();
      _cameraController.dispose();
    }
    
    _faceDetector.close();
    super.dispose();
  }

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      backgroundColor: Colors.black,
      appBar: AppBar(
        backgroundColor: Colors.black,
        foregroundColor: Colors.white,
        elevation: 0,
        title: Text('Verification ${_currentActionIndex + 1}/${_challengeActions.length}'),
        centerTitle: true,
        actions: [
          Padding(
            padding: const EdgeInsets.only(right: 16),
            child: Center(
              child: Text(
                '$_timeRemaining s',
                style: TextStyle(
                  color: _timeRemaining <= 10 ? Colors.red : Colors.white,
                  fontWeight: FontWeight.bold,
                ),
              ),
            ),
          ),
        ],
      ),
      body: _isCameraInitialized
          ? Stack(
              children: [
                // Camera Preview
                Positioned.fill(
                  child: CameraPreview(_cameraController),
                ),
                
                // Face Overlay
                if (_faceBoundingBox != null)
                  CustomPaint(
                    painter: FaceOverlayPainter(_faceBoundingBox!),
                    child: Container(),
                  ),
                
                // Head Mask
                CustomPaint(
                  painter: HeadMaskPainter(_animationController.value),
                  child: Container(),
                ),
                
                // Instruction Overlay
                Positioned(
                  top: 40,
                  left: 16,
                  right: 16,
                  child: Container(
                    padding: const EdgeInsets.all(16),
                    decoration: BoxDecoration(
                      color: Colors.black.withOpacity(0.7),
                      borderRadius: BorderRadius.circular(12),
                    ),
                    child: Column(
                      children: [
                        Text(
                          _getCurrentInstructionText(),
                          style: const TextStyle(
                            color: Colors.white,
                            fontSize: 20,
                            fontWeight: FontWeight.bold,
                          ),
                          textAlign: TextAlign.center,
                        ),
                        const SizedBox(height: 8),
                        LinearProgressIndicator(
                          value: (_currentActionIndex + 1) / _challengeActions.length,
                          backgroundColor: Colors.grey.shade600,
                          valueColor: AlwaysStoppedAnimation<Color>(
                            _challengeCompleted ? Colors.green : Colors.blue,
                          ),
                        ),
                      ],
                    ),
                  ),
                ),
                
                // Debug Information (Development Only)
                if (kDebugMode)
                  Positioned(
                    bottom: 100,
                    left: 16,
                    child: Container(
                      padding: const EdgeInsets.all(8),
                      decoration: BoxDecoration(
                        color: Colors.black.withOpacity(0.7),
                        borderRadius: BorderRadius.circular(8),
                      ),
                      child: Column(
                        crossAxisAlignment: CrossAxisAlignment.start,
                        mainAxisSize: MainAxisSize.min,
                        children: [
                          Text(
                            'Debug Info:',
                            style: TextStyle(color: Colors.white, fontWeight: FontWeight.bold),
                          ),
                          Text(
                            'Smile: ${(_smilingProbability ?? 0).toStringAsFixed(2)}',
                            style: TextStyle(color: Colors.white, fontSize: 12),
                          ),
                          Text(
                            'L Eye: ${(_leftEyeOpenProbability ?? 0).toStringAsFixed(2)}',
                            style: TextStyle(color: Colors.white, fontSize: 12),
                          ),
                          Text(
                            'R Eye: ${(_rightEyeOpenProbability ?? 0).toStringAsFixed(2)}',
                            style: TextStyle(color: Colors.white, fontSize: 12),
                          ),
                          Text(
                            'Head Y: ${(_headEulerAngleY ?? 0).toStringAsFixed(1)}°',
                            style: TextStyle(color: Colors.white, fontSize: 12),
                          ),
                        ],
                      ),
                    ),
                  ),
                
                // Cancel Button
                Positioned(
                  bottom: 40,
                  left: 16,
                  right: 16,
                  child: ElevatedButton(
                    onPressed: () => Navigator.of(context).pop(false),
                    style: ElevatedButton.styleFrom(
                      backgroundColor: Colors.red.withOpacity(0.8),
                      foregroundColor: Colors.white,
                      padding: const EdgeInsets.symmetric(vertical: 16),
                      shape: RoundedRectangleBorder(
                        borderRadius: BorderRadius.circular(12),
                      ),
                    ),
                    child: const Text('Cancel Verification'),
                  ),
                ),
              ],
            )
          : const Center(
              child: Column(
                mainAxisAlignment: MainAxisAlignment.center,
                children: [
                  CircularProgressIndicator(color: Colors.white),
                  SizedBox(height: 16),
                  Text(
                    'Initializing camera...',
                    style: TextStyle(color: Colors.white),
                  ),
                ],
              ),
            ),
    );
  }
}

// Custom Painters
class FaceOverlayPainter extends CustomPainter {
  final Rect boundingBox;

  FaceOverlayPainter(this.boundingBox);

  @override
  void paint(Canvas canvas, Size size) {
    final paint = Paint()
      ..color = Colors.green
      ..style = PaintingStyle.stroke
      ..strokeWidth = 3.0;

    // Draw bounding box
    canvas.drawRect(boundingBox, paint);
    
    // Draw corner markers
    final cornerLength = 20.0;
    final corners = [
      boundingBox.topLeft,
      boundingBox.topRight,
      boundingBox.bottomLeft,
      boundingBox.bottomRight,
    ];

    for (final corner in corners) {
      canvas.drawLine(
        corner,
        Offset(corner.dx + cornerLength, corner.dy),
        paint..strokeWidth = 4.0,
      );
      canvas.drawLine(
        corner,
        Offset(corner.dx, corner.dy + cornerLength),
        paint,
      );
    }
  }

  @override
  bool shouldRepaint(covariant CustomPainter oldDelegate) => true;
}

class HeadMaskPainter extends CustomPainter {
  final double animationValue;

  HeadMaskPainter(this.animationValue);

  @override
  void paint(Canvas canvas, Size size) {
    final paint = Paint()
      ..color = Colors.black.withOpacity(0.6)
      ..style = PaintingStyle.fill;

    final center = Offset(size.width / 2, size.height / 2 - 50);
    final radius = size.width * 0.35 + (animationValue * 10);

    final path = Path()
      ..addRect(Rect.fromLTWH(0, 0, size.width, size.height))
      ..addOval(Rect.fromCircle(center: center, radius: radius))
      ..fillType = PathFillType.evenOdd;

    canvas.drawPath(path, paint);

    // Draw guide circle
    final guidePaint = Paint()
      ..color = Colors.white.withOpacity(0.3 + animationValue * 0.2)
      ..style = PaintingStyle.stroke
      ..strokeWidth = 2.0;

    canvas.drawCircle(center, radius, guidePaint);
  }

  @override
  bool shouldRepaint(covariant CustomPainter oldDelegate) => true;
}
```

### 3. Enhanced Liveness Detection Service

```dart
// liveness_detection_service.dart
import 'dart:async';
import 'dart:math';
import 'package:google_mlkit_face_detection/google_mlkit_face_detection.dart';

class LivenessDetectionService {
  static const double _smileThreshold = 0.7;
  static const double _eyeClosedThreshold = 0.3;
  static const double _headTurnThreshold = 15.0;
  static const double _neutralThreshold = 0.3;
  static const int _consecutiveFramesRequired = 5;
  static const int _maxTimePerChallenge = 10; // seconds

  final Map<String, int> _consecutiveFrames = {};
  final Map<String, Timer> _challengeTimers = {};
  
  void resetCounters() {
    _consecutiveFrames.clear();
    _challengeTimers.forEach((key, timer) => timer.cancel());
    _challengeTimers.clear();
  }

  bool validateChallenge({
    required String action,
    required Face face,
    required Function(String) onTimeout,
  }) {
    // Start timeout timer for this challenge if not already started
    if (!_challengeTimers.containsKey(action)) {
      _challengeTimers[action] = Timer(
        Duration(seconds: _maxTimePerChallenge),
        () => onTimeout(action),
      );
    }

    bool isValid = false;

    switch (action) {
      case 'neutral':
        isValid = _isNeutralPosition(face);
        break;
      case 'smile':
        isValid = _isSmiling(face);
        break;
      case 'blink':
        isValid = _isBlinking(face);
        break;
      case 'lookLeft':
        isValid = _isLookingLeft(face);
        break;
      case 'lookRight':
        isValid = _isLookingRight(face);
        break;
      case 'nod':
        isValid = _isNodding(face);
        break;
    }

    if (isValid) {
      _consecutiveFrames[action] = (_consecutiveFrames[action] ?? 0) + 1;
      
      if (_consecutiveFrames[action]! >= _consecutiveFramesRequired) {
        _challengeTimers[action]?.cancel();
        _challengeTimers.remove(action);
        _consecutiveFrames.remove(action);
        return true;
      }
    } else {
      _consecutiveFrames[action] = 0;
    }

    return false;
  }

  bool _isNeutralPosition(Face face) {
    final smiling = face.smilingProbability ?? 0.0;
    final leftEye = face.leftEyeOpenProbability ?? 1.0;
    final rightEye = face.rightEyeOpenProbability ?? 1.0;
    final headY = (face.headEulerAngleY ?? 0.0).abs();
    final headX = (face.headEulerAngleX ?? 0.0).abs();

    return smiling < _neutralThreshold &&
           leftEye > 0.7 &&
           rightEye > 0.7 &&
           headY < 10.0 &&
           headX < 15.0;
  }

  bool _isSmiling(Face face) {
    return (face.smilingProbability ?? 0.0) > _smileThreshold;
  }

  bool _isBlinking(Face face) {
    final leftEye = face.leftEyeOpenProbability ?? 1.0;
    final rightEye = face.rightEyeOpenProbability ?? 1.0;
    
    return leftEye < _eyeClosedThreshold || rightEye < _eyeClosedThreshold;
  }

  bool _isLookingLeft(Face face) {
    return (face.headEulerAngleY ?? 0.0) > _headTurnThreshold;
  }

  bool _isLookingRight(Face face) {
    return (face.headEulerAngleY ?? 0.0) < -_headTurnThreshold;
  }

  bool _isNodding(Face face) {
    return (face.headEulerAngleX ?? 0.0).abs() > 20.0;
  }

  double getConfidenceScore(Face face, String action) {
    switch (action) {
      case 'smile':
        return face.smilingProbability ?? 0.0;
      case 'blink':
        final leftEye = face.leftEyeOpenProbability ?? 1.0;
        final rightEye = face.rightEyeOpenProbability ?? 1.0;
        return 1.0 - ((leftEye + rightEye) / 2);
      case 'lookLeft':
        final angle = face.headEulerAngleY ?? 0.0;
        return angle > 0 ? angle / 45.0 : 0.0;
      case 'lookRight':
        final angle = face.headEulerAngleY ?? 0.0;
        return angle < 0 ? (-angle) / 45.0 : 0.0;
      default:
        return 0.0;
    }
  }

  void dispose() {
    resetCounters();
  }
}
```

## Security Considerations

### Anti-Spoofing Measures

1. **Multiple Challenge Verification**
   - Randomize challenge order
   - Require sequential completion
   - Validate smooth transitions between actions

2. **Temporal Analysis**
   - Monitor face tracking continuity
   - Detect unnatural movements
   - Validate response timing

3. **Quality Checks**
   - Minimum face size requirements
   - Lighting condition validation
   - Image sharpness verification

```dart
// Anti-spoofing utilities
class AntiSpoofingValidator {
  static const int _minTrackingFrames = 30;
  static const double _minFaceSize = 0.3;
  static const double _maxMovementSpeed = 50.0; // pixels per frame
  
  final List<Rect> _faceHistory = [];
  final List<double> _movementSpeeds = [];
  int? _lastTrackingId;
  int _consecutiveTrackingFrames = 0;

  bool validateFrame(Face face, Size imageSize) {
    // Validate face size
    final faceArea = face.boundingBox.width * face.boundingBox.height;
    final imageArea = imageSize.width * imageSize.height;
    final faceRatio = faceArea / imageArea;
    
    if (faceRatio < _minFaceSize) {
      return false;
    }

    // Validate tracking consistency
    if (_lastTrackingId != null && face.trackingId != _lastTrackingId) {
      _consecutiveTrackingFrames = 0;
    } else {
      _consecutiveTrackingFrames++;
    }
    
    _lastTrackingId = face.trackingId;

    // Track movement patterns
    if (_faceHistory.isNotEmpty) {
      final lastRect = _faceHistory.last;
      final movement = _calculateMovement(lastRect, face.boundingBox);
      _movementSpeeds.add(movement);
      
      // Keep only recent movements
      if (_movementSpeeds.length > 10) {
        _movementSpeeds.removeAt(0);
      }
      
      // Validate movement is not too erratic
      if (movement > _maxMovementSpeed) {
        return false;
      }
    }

    _faceHistory.add(face.boundingBox);
    if (_faceHistory.length > 30) {
      _faceHistory.removeAt(0);
    }

    return _consecutiveTrackingFrames >= _minTrackingFrames;
  }

  double _calculateMovement(Rect previous, Rect current) {
    final dx = previous.center.dx - current.center.dx;
    final dy = previous.center.dy - current.center.dy;
    return sqrt(dx * dx + dy * dy);
  }

  void reset() {
    _faceHistory.clear();
    _movementSpeeds.clear();
    _lastTrackingId = null;
    _consecutiveTrackingFrames = 0;
  }
}
```

## Performance Optimization

### 1. Frame Processing Optimization
```dart
class FrameProcessor {
  static const int _skipFrames = 2; // Process every 3rd frame
  int _frameCount = 0;
  
  bool shouldProcessFrame() {
    _frameCount++;
    return _frameCount % (_skipFrames + 1) == 0;
  }
}
```

### 2. Memory Management
```dart
class ResourceManager {
  Timer? _cleanupTimer;
  
  void startCleanup() {
    _cleanupTimer = Timer.periodic(const Duration(seconds: 5), (_) {
      // Force garbage collection periodically
      if (Platform.isAndroid) {
        SystemChannels.platform.invokeMethod('System.gc');
      }
    });
  }
  
  void dispose() {
    _cleanupTimer?.cancel();
  }
}
```

### 3. Camera Configuration
```dart
// Optimal camera settings for face detection
final cameraController = CameraController(
  frontCamera,
  ResolutionPreset.medium, // Balance between quality and performance
  enableAudio: false,
  imageFormatGroup: Platform.isAndroid 
    ? ImageFormatGroup.nv21 
    : ImageFormatGroup.bgra8888,
);
```

## Troubleshooting

### Common Issues and Solutions

#### 1. **Camera Initialization Fails**
```dart
// Solution: Add proper error handling
try {
  await _cameraController.initialize();
} on CameraException catch (e) {
  switch (e.code) {
    case 'CameraAccessDenied':
      // Request permission again
      break;
    case 'CameraAccessDeniedWithoutPrompt':
      // Direct user to settings
      break;
    default:
      // Log error and fallback
      break;
  }
}
```

#### 2. **Face Detection Not Working**
- Ensure adequate lighting
- Check face size requirements (minimum 100x100 pixels)
- Verify camera permissions
- Test on physical device (not simulator)

#### 3. **Poor Performance**
- Reduce camera resolution
- Skip frames during processing
- Use FAST mode for FaceDetectorOptions
- Implement proper resource cleanup

#### 4. **iOS Build Issues**
```bash
# Clean and rebuild
flutter clean
cd ios
pod deintegrate
pod install
cd ..
flutter build ios
```

## Best Practices

### 1. **User Experience**
- Provide clear instructions
- Show progress indicators
- Handle edge cases gracefully
- Offer retry mechanisms

### 2. **Security**
- Implement timeout mechanisms
- Use randomized challenge sequences
- Validate tracking consistency
- Monitor for suspicious patterns

### 3. **Performance**
- Process frames efficiently
- Manage memory usage
- Implement proper cleanup
- Use appropriate camera settings

### 4. **Error Handling**
```dart
class LivenessErrorHandler {
  static void handleError(dynamic error, StackTrace stackTrace) {
    // Log error
    debugPrint('Liveness Error: $error');
    
    // Report to analytics
    FirebaseCrashlytics.instance.recordError(error, stackTrace);
    
    // Show user-friendly message
    // Handle specific error types
  }
}
```

### 5. **Testing Strategy**
- Test on various devices
- Validate under different lighting conditions
- Test with different face angles
- Verify timeout mechanisms
- Test error scenarios

This comprehensive guide provides everything needed to implement robust live facial liveness detection in Flutter using ML Kit. The implementation includes security measures, performance optimizations, and proper error handling for production use.

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------


📊 Current Status:
✅ Face positioning detection enhanced
✅ Visual feedback system implemented
✅ Challenge system optimized
✅ UI/UX improvements completed
🔄 Next: Anti-spoofing measures and performance optimization
Would you like me to continue with STEP 5: Anti-Spoofing Measures and STEP 6: Performance Optimization, or would you prefer to test the current implementation first, senpai? (◕‿◕)


---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

so i requested assistance from claude to figure out why the detection may not be working heres the conversation in it is its advice 
can we try this?


I'm trying to implement a face detection service for my liveness check implementation 
I want to know in simple terms how should I detect the face using a constant stream of images with mlkit and such in flutter

I'll explain a simple approach for face detection in Flutter using ML Kit for your liveness check implementation.

Basic Architecture
Stream Processing Flow:

Camera stream → Frame capture → ML Kit face detection → Process results
Use a timer or frame rate limiter to avoid processing every single frame (expensive)
Process results to determine face presence, position, and quality
Implementation Approach
1. Camera Setup

dart
// Use camera plugin to get image stream
CameraController controller;
controller.startImageStream((CameraImage image) {
  // Process frames here (but not every frame!)
});
2. Frame Rate Control

dart
bool _isProcessing = false;

void processFrame(CameraImage image) {
  if (_isProcessing) return; // Skip if still processing previous frame
  _isProcessing = true;
  
  // Process with ML Kit
  detectFaces(image).then((_) {
    _isProcessing = false;
  });
}
3. ML Kit Face Detection

dart
final FaceDetector faceDetector = FaceDetector(
  options: FaceDetectorOptions(
    enableLandmarks: true,
    enableClassification: true,
    minFaceSize: 0.1, // Minimum face size relative to image
  ),
);

Future<void> detectFaces(CameraImage image) async {
  final inputImage = InputImage.fromBytes(
    bytes: image.planes[0].bytes,
    metadata: InputImageMetadata(/* camera metadata */),
  );
  
  final List<Face> faces = await faceDetector.processImage(inputImage);
  
  // Process detected faces
  handleDetectionResults(faces);
}
4. Liveness Check Logic For basic liveness detection, check for:

Single face present (faces.length == 1)
Face size (face.boundingBox.width > minSize)
Face position (roughly centered)
Face orientation (using head euler angles)
Eye state (if classification enabled, check if eyes are open)
5. Performance Tips

Process every 3rd-5th frame, not every frame
Use lower camera resolution for detection (e.g., 640x480)
Consider using FaceDetectorOptions.performanceMode = fast
Dispose detector properly to avoid memory leaks
Key Considerations:

ML Kit works offline, which is great for privacy
Face detection ≠ face recognition (ML Kit doesn't identify who the person is)
For production liveness checks, you'll likely need additional anti-spoofing measures beyond basic face detection
Consider device performance - older devices may struggle with real-time processing
This gives you a solid foundation for face detection in your liveness check. Would you like me to elaborate on any specific part, like the camera setup or the actual liveness validation logic?


So are you sure this will enable me to detect faces?

Yes, this approach will definitely detect faces, but let me clarify what you'll actually get and potential gotchas:

What You'll Get
Face bounding boxes (rectangular area where face is located)
Facial landmarks (eyes, nose, mouth positions if enabled)
Head rotation angles (pitch, yaw, roll)
Face classifications (eyes open/closed, smiling probability if enabled)
Confidence scores for detections
What Actually Works
ML Kit's face detection is quite reliable for:

Frontal and slightly angled faces
Good lighting conditions
Faces that aren't too small in the frame
Static and moving faces
Common Issues You Might Hit
1. Camera Image Format

dart
// You need to handle different image formats properly
InputImage inputImage;
if (Platform.isIOS) {
  inputImage = InputImage.fromBytes(
    bytes: image.planes[0].bytes,
    metadata: InputImageMetadata(
      size: Size(image.width.toDouble(), image.height.toDouble()),
      rotation: InputImageRotation.rotation0deg, // Critical!
      format: InputImageFormat.bgra8888,
      bytesPerRow: image.planes[0].bytesPerRow,
    ),
  );
} else {
  // Android YUV format handling is more complex
}
2. Performance Reality Check

First detection might take 500-1000ms
Subsequent detections: 50-200ms on modern devices
Older devices can be much slower
3. Detection Limitations

Won't detect faces smaller than minFaceSize threshold
Struggles with extreme angles, poor lighting, or partial occlusion
May have false positives/negatives
Quick Test
To verify it works, start with a simple static image test first:

dart
final inputImage = InputImage.fromFilePath(imagePath);
final faces = await faceDetector.processImage(inputImage);
print('Detected ${faces.length} faces');
The approach I outlined will work, but you'll need to handle the camera format conversion properly and tune the detection parameters for your specific use case. Have you already set up the camera stream, or are you starting from scratch?



face_detect.dart
968 lines

dart

so i have something written but i dont know how to tell if its detecting my face or not 
from the ui perspective its cleary not cuz even in the best lithing its still not changing states or doing anything so im unsure if its an issue with the code or the ui please can you help ??

Looking at your code, I can see several potential issues that could prevent face detection from working. Let me help you debug this step by step.

Most Likely Issues
1. Image Format & Rotation Problems Your current image conversion looks suspicious:

dart
// This approach might not work correctly
final WriteBuffer allBytes = WriteBuffer();
for (Plane plane in image.planes) {
  allBytes.putUint8List(plane.bytes);
}
2. Missing Import I don't see the ML Kit imports in your file. You need:

dart
import 'package:google_ml_kit/google_ml_kit.dart';
Debug Steps

Face Detection Debug Helper
Code 
Quick Fixes to Try
1. Replace your detectFaces method temporarily:

dart
// Replace your detectFaces method with detectFacesDebug
Future<void> detectFaces(CameraImage image) async {
  return detectFacesDebug(image);  // Use the debug version
}
2. Test ML Kit setup first: Add this to your initState():

dart
@override
void initState() {
  super.initState();
  testMLKitSetup(); // Add this line
  // ... rest of your init code
}
3. Add required imports: Make sure you have these imports at the top of your file:

dart
import 'package:google_ml_kit/google_ml_kit.dart';
import 'dart:io' show Platform;
import 'dart:typed_data';
4. Verify your pubspec.yaml:

yaml
dependencies:
  google_ml_kit: ^0.18.0  # or latest version
  camera: ^0.10.5
What to Look For
Run your app and check the debug console (in Android Studio/VS Code). You should see:

Face detection logs starting with 🔍, 📏, etc.
Either "👥 Detected faces: X" or "❌ No faces detected"
Any error messages with 💥
If you see errors about:

"InputImage creation failed" → Image format issue
"Face detector not initialized" → ML Kit setup issue
"Platform exception" → Permissions or camera issue
If you see "No faces detected" consistently:

Try different lighting
Make sure your face fills about 30-50% of the screen
Check if the camera preview is showing correctly
Try this debug version first and let me know what the console output shows. This will tell us exactly where the problem is!


im using barrel imports so its not a import issue

Got it, with barrel imports that's not the issue. Let's focus on the actual detection problems then.

Most Likely Issues
1. Image Format Conversion Problem Your current approach of concatenating all planes might be incorrect for Android YUV format:

dart
// This might be wrong for YUV420/NV21
final WriteBuffer allBytes = WriteBuffer();
for (Plane plane in image.planes) {
  allBytes.putUint8List(plane.bytes);
}
2. Camera Image Format Mismatch You're setting ImageFormatGroup.yuv420 but telling ML Kit it's InputImageFormat.nv21.

3. Rotation Issues Front camera rotation is tricky - you might need different rotation values.

Quick Test
Replace your detectFaces method with this simplified version to isolate the issue:

dart
Future<void> detectFaces(CameraImage image) async {
  try {
    print('🔍 Image: ${image.width}x${image.height}, format: ${image.format.group}');
    
    // Try the simplest approach first
    final inputImage = InputImage.fromBytes(
      bytes: image.planes[0].bytes, // Use only first plane
      metadata: InputImageMetadata(
        size: Size(image.width.toDouble(), image.height.toDouble()),
        rotation: InputImageRotation.rotation0deg, // Try 0° first
        format: InputImageFormat.yuv420, // Match your camera format
        bytesPerRow: image.planes[0].bytesPerRow,
      ),
    );

    final faces = await faceDetector.processImage(inputImage);
    print('👥 Found ${faces.length} faces');

    if (faces.isNotEmpty) {
      print('✅ FACE DETECTED! Bounding box: ${faces.first.boundingBox}');
      setState(() {
        isFaceInFrame = true;
        facePositionFeedback = 'Face detected! Debug successful';
      });
    } else {
      setState(() {
        isFaceInFrame = false;
        facePositionFeedback = 'No face - check lighting/position';
      });
    }
  } catch (e) {
    print('💥 Error: $e');
    setState(() {
      facePositionFeedback = 'Error: $e';
    });
  }
}
Also Try This Camera Setup
dart
cameraController = CameraController(
  frontCamera,
  ResolutionPreset.medium, // Try medium instead of max
  enableAudio: false,
  imageFormatGroup: ImageFormatGroup.bgra8888, // Try this format
);
If you use bgra8888, then change your detection to:

dart
format: InputImageFormat.bgra8888,
Debug Questions
Are you seeing the print statements in the console? (This tells us if detectFaces is even being called)
What does the console show for image dimensions and format?
Is the camera preview showing your face clearly?
Are you testing on Android or iOS? (The image format handling is different)
Try the simplified version first and tell me what the console output shows. If we can get even one face detected, then we know ML Kit is working and it's just a parameter tuning issue.






// Add this method to your _FaceDetectionViewState class to debug

/// Debug method to test face detection with proper error handling
Future<void> detectFacesDebug(CameraImage image) async {
  print('🔍 Starting face detection...');
  print('📏 Image dimensions: ${image.width}x${image.height}');
  print('📱 Image format: ${image.format.group}');
  print('✈️ Image planes: ${image.planes.length}');
  
  try {
    InputImage inputImage;
    
    // Platform-specific image conversion
    if (Platform.isIOS) {
      inputImage = InputImage.fromBytes(
        bytes: image.planes[0].bytes,
        metadata: InputImageMetadata(
          size: Size(image.width.toDouble(), image.height.toDouble()),
          rotation: InputImageRotation.rotation0deg,
          format: InputImageFormat.bgra8888,
          bytesPerRow: image.planes[0].bytesPerRow,
        ),
      );
    } else {
      // Android - YUV format handling
      inputImage = InputImage.fromBytes(
        bytes: _concatenatePlanes(image.planes),
        metadata: InputImageMetadata(
          size: Size(image.width.toDouble(), image.height.toDouble()),
          rotation: _getRotation(),
          format: InputImageFormat.nv21,
          bytesPerRow: image.planes[0].bytesPerRow,
        ),
      );
    }

    print('🖼️ InputImage created successfully');
    
    // Process image for face detection
    final faces = await faceDetector.processImage(inputImage);
    
    print('👥 Detected faces: ${faces.length}');
    
    if (faces.isNotEmpty) {
      final face = faces.first;
      print('📦 Face bounding box: ${face.boundingBox}');
      print('😊 Smiling probability: ${face.smilingProbability}');
      print('👀 Left eye open: ${face.leftEyeOpenProbability}');
      print('👀 Right eye open: ${face.rightEyeOpenProbability}');
      print('🔄 Head angles - Y: ${face.headEulerAngleY}, X: ${face.headEulerAngleX}, Z: ${face.headEulerAngleZ}');
      print('🆔 Tracking ID: ${face.trackingId}');
      
      // Update UI state
      updateFaceDetectionResults(face, image.width, image.height);
      checkFacePositioning(face, image.width, image.height);
      
      print('✅ Face detection completed successfully');
    } else {
      print('❌ No faces detected');
      setState(() {
        isFaceInFrame = false;
        isFaceCentered = false;
        isFaceLookingStraight = false;
        isFaceQualityGood = false;
        facePositionFeedback = 'No face detected - Debug: Check lighting and positioning';
      });
    }
    
  } catch (e, stackTrace) {
    print('💥 Face detection error: $e');
    print('📍 Stack trace: $stackTrace');
    
    // Update UI with error
    if (mounted) {
      setState(() {
        facePositionFeedback = 'Detection error: ${e.toString()}';
      });
    }
  }
}

// Helper method to concatenate image planes for Android
Uint8List _concatenatePlanes(List<Plane> planes) {
  final WriteBuffer allBytes = WriteBuffer();
  for (Plane plane in planes) {
    allBytes.putUint8List(plane.bytes);
  }
  return allBytes.done().buffer.asUint8List();
}

// Helper method to get correct rotation
InputImageRotation _getRotation() {
  // For front camera, typically needs 270° rotation
  // You might need to adjust this based on your device orientation
  return InputImageRotation.rotation270deg;
}

// Add this test method to validate ML Kit setup
Future<void> testMLKitSetup() async {
  try {
    print('🧪 Testing ML Kit setup...');
    
    // Test with a simple detector creation
    final testDetector = FaceDetector(
      options: FaceDetectorOptions(
        performanceMode: FaceDetectorMode.fast,
        enableLandmarks: false,
        enableClassification: false,
        minFaceSize: 0.1,
      ),
    );
    
    print('✅ Face detector created successfully');
    await testDetector.close();
    print('✅ Face detector disposed successfully');
    print('🎉 ML Kit setup is working!');
    
  } catch (e) {
    print('💥 ML Kit setup error: $e');
    print('❌ ML Kit may not be properly configured');
  }
}



-------------------------------------------------------------------------------------------------------------------------------------------------------------
AFTER ACCOMPLISHING THE FINAL FEATURE WITH EVERYTHING I WANT A COMPLETE GUIDE ON HOW TO IMPLEMENT LIVENESS CHECK IN FLUTTER USING ML KIT